{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.path.split(os.getcwd())[0])\n",
    "sys.path.append(os.path.split(os.path.split(os.path.split(os.getcwd())[0])[0])[0]) # master drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from nhits_pl import NHITS\n",
    "from typing import Dict,List,Tuple\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if is_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 500\n",
    "n_classes = 4\n",
    "# Prepare sample data\n",
    "timestamp = pd.date_range(start='2020-01-01', periods=n_rows, freq='D')\n",
    "time_series = pd.DataFrame({'values': np.random.randn(n_rows)}, index=timestamp)\n",
    "labels = pd.DataFrame({'label': np.random.randn(n_rows)}, index=timestamp)\n",
    "ext_features = pd.DataFrame({\n",
    "    'feature1': np.random.randn(n_rows),\n",
    "    'feature2': np.random.randn(n_rows)\n",
    "}, index=timestamp)\n",
    "\n",
    "combined_data = time_series.join(labels).join(ext_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 500 entries, 2020-01-01 to 2021-05-14\n",
      "Freq: D\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   values    500 non-null    float64\n",
      " 1   label     500 non-null    float64\n",
      " 2   feature1  500 non-null    float64\n",
      " 3   feature2  500 non-null    float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 35.7 KB\n"
     ]
    }
   ],
   "source": [
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch: NHITS Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define batch size and number of historical steps (to model) and future steps (to forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch\n",
    "batch_size = 256\n",
    "historical_steps = 90\n",
    "future_steps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read exogenous features and target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = combined_data.loc[:,['feature1', 'feature2']]\n",
    "label = combined_data['label'][-(len(combined_data)-historical_steps):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch: Data Setup (Static / Future / Observed)\n",
    "\n",
    "`Please note: at the moment we're filling the static covariates with random data, but this may need to be changed in future to adjust for eg. business days / holidays`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tft_train_utils import stack_past_values\n",
    "n_obs = n_rows\n",
    "batch = {\n",
    "        # Seasonal Component\n",
    "        'seasonality' : torch.tensor([30], dtype=torch.int32), # for monthly seasonality\n",
    "        # Input Features\n",
    "        'X': torch.randn((n_obs, 24, 5), dtype=torch.float32),\n",
    "\n",
    "        '''sample_mask is used to specify which parts of the input data should be considered for loss calculation or predictions. \n",
    "        It helps in ignoring padding or missing values in the input sequences during training'''\n",
    "        'sample_mask': torch.ones((64, 24)),  # Mask with 1s for valid data, 0s for padding/missing\n",
    "\n",
    "\n",
    "        '''available_mask is used to indicate the availability of data in the sequence. \n",
    "        It is crucial for handling missing data points effectively during both training and evaluation'''\n",
    "        'avaliable_mask': torch.ones((64, 24)),  # Mask with 1s for valid data, 0s for padding/missing\n",
    "\n",
    "        'y' : torch.tensor(label[-(n_obs):].values, dtype=torch.int64)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch - NHITS Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'time_in' : torch.arange(-24, 0),  # Last 24 time steps as input context,\n",
    "    'time_out' : torch.arange(0, 1),  # Next 1 time steps to predict\n",
    "    'n_pool_kernel' : [2,2,1], # The pooling size for downsampling in each stack. Higher values lead to more smoothing of the input.\n",
    "    'n_freq_downsample' : [2, 2, 1], # Specifies the downsampling factor for each stack, determining how much interpolation is required (should be equal or higher than n_pool_kernel)\n",
    "    'hidden_size': 128, # The number of units in the hidden layers.\n",
    "    'n_blocks' : [3, 3, 3], # Number of blocks in each stack, which controls the depth of the network.\n",
    "    'n_layers' : 2, # Number of layers within each block.\n",
    "    'downsample_frequencies' : [2,2,1], # Multiplier for downsampling output in each stack. It should match the n_freq_downsample parameter.\n",
    "    'batch_normalization': False, # Whether to apply batch normalization after each block.\n",
    "    'dropout' : 0.5,\n",
    "    'activation': 'ReLU',\n",
    "    'learning_rate' : 1e-4,\n",
    "    'loss' : MSEloss(),\n",
    "    'log_interval' : 100, # Interval for logging model performance\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch-Lightning: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create instance of NHITS Pytorch-Ligthning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NHITS(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compile dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "S = batch['seasonality']\n",
    "X = batch['X']\n",
    "y = batch['y']\n",
    "sample_mask = batch['sample_mask']\n",
    "available_mask = batch['available_mask']\n",
    "\n",
    "train_data = TensorDataset(\n",
    "    S, X, y, sample_mask, available_mask)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run the first small training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "from sklearn.metrics import log_loss\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.nn import MSELoss\n",
    "from CPCV.cpcv import CombinatorialPurgedGroupKFold\n",
    "\n",
    "def tft_objective(trial):\n",
    "\n",
    "    vars = ['seasonality', 'X', 'y', 'sample_mask', 'available_mask']\n",
    "\n",
    "    config = model_config\n",
    "    dataset = train_data\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_categorical('lr', [1e-5, 1e-3, 1e-2])\n",
    "    num_heads = trial.suggest_categorical('num_heads', [1, 2, 4])\n",
    "    dropout_prob = trial.suggest_categorical('dropout_prob', [0.1, 0.3, 0.5])\n",
    "    hidden_units = trial.suggest_categorical('hidden_units', [64, 128, 256])\n",
    "    lstm_layers = trial.suggest_categorical('lstm_layers', [1, 2, 4])\n",
    "    # classifier_units = trial.suggest_categorical('classifier_units', [16, 32, 64])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "\n",
    "    config['optimization']['learning_rate'] = lr\n",
    "    config['optimization']['batch_size'] = batch_size\n",
    "    config['model']['dropout'] = dropout_prob\n",
    "    config['model']['hidden_size'] = hidden_units\n",
    "    # config['model']['lstm_layers'] = lstm_layers\n",
    "    # config['model']['attention_heads'] = num_heads\n",
    "\n",
    "    # Initialize the model with suggested hyperparameters\n",
    "    model = NHITS(config=OmegaConf.create(config))\n",
    "\n",
    "    # Time series split\n",
    "    n_splits = 5\n",
    "    n_test_splits = 1\n",
    "\n",
    "    cpcv = CombinatorialPurgedGroupKFold(\n",
    "        n_splits=5,\n",
    "        n_test_splits=1,\n",
    "        pctEmbargo=0.01\n",
    "    )\n",
    "\n",
    "    groups = [i // n_splits for i in range(len(dataset))]\n",
    "    cv_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cpcv.split(dataset, groups=groups)):\n",
    "\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=10,\n",
    "            callbacks=[EarlyStopping(monitor='train_loss', patience=5, mode='min')],\n",
    "            logger=False,\n",
    "            enable_checkpointing=False,\n",
    "            enable_model_summary=False\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.fit(model, train_loader)\n",
    "\n",
    "        # Validate the model\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {v:i for v,i in zip(vars, batch)}\n",
    "                logits = model(batch)[0]\n",
    "                # preds = torch.argmax(classification, dim=1)\n",
    "                all_preds.extend(logits.squeeze(1).cpu().numpy())\n",
    "                all_targets.extend(batch['y'].flatten().cpu().numpy())\n",
    "\n",
    "        # val_predictions = trainer.predict(model, val_loader)\n",
    "        # val_predictions = torch.cat([x for x in val_predictions], dim=0).numpy()\n",
    "        \n",
    "        val_loss = MSELoss()(torch.tensor(all_preds), torch.tensor(all_targets))\n",
    "        cv_scores.append(val_loss)\n",
    "\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run hyperparameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(tft_objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read the best set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['learning_rate'] = study.best_params['lr']\n",
    "model_config['batch_size'] = study.best_params['batch_size']\n",
    "model_config['dropout'] = study.best_params['dropout_prob']\n",
    "model_config['hidden_size'] = study.best_params['hidden_units']\n",
    "# model_config['model']['lstm_layers'] = study.best_params['lstm_layers']\n",
    "# model_config['model']['attention_heads'] = study.best_params['num_heads']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NHITS(config=OmegaConf.create(model_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    callbacks=[EarlyStopping(monitor='train_loss', patience=10, mode='min')]\n",
    "    ) # remember to add the callbacks\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: Perform Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CPCV.cpcv import CombinatorialPurgedGroupKFold\n",
    "from tft_train_utils import cross_validate_model\n",
    "\n",
    "date_index = combined_data[historical_steps:].index\n",
    "\n",
    "pred_times = pd.Series(date_index, index=date_index)\n",
    "eval_times = pd.Series(date_index, index=date_index)\n",
    "\n",
    "# Construct CPCV in-line with DePrado method\n",
    "cpcv = CombinatorialPurgedGroupKFold(\n",
    "    n_splits=10,\n",
    "    n_test_splits=1,\n",
    "    pctEmbargo = 0.01\n",
    ")\n",
    "\n",
    "cv_split = cpcv.split(\n",
    "    X=pd.DataFrame(X, index=date_index), \n",
    "    y=pd.Series(y, index=date_index), \n",
    "    groups = [i // 10 for i in range(len(X))])\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate_model(\n",
    "    train_data,\n",
    "    model=model,\n",
    "    num_epochs=10,\n",
    "    # num_classes=data_props['num_classes'],\n",
    "    cv_split=cv_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: MC Dropout Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mc_dropout_tft import mc_dropout\n",
    "mean_predictions, std_predictions = mc_dropout(model, train_dataloader, mc_iterations=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Export the reuslts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Save test predictions to a CSV\n",
    "test_df = pd.DataFrame({\n",
    "    'Prediction': mean_predictions,\n",
    "    'Uncertainty': std_predictions\n",
    "})\n",
    "\n",
    "test_df.to_csv('nhits_predictions_reg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "save(model.state_dict(), 'nhits_regressor.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GluonTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

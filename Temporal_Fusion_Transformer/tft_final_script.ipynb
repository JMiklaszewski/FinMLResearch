{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.path.split(os.getcwd())[0])\n",
    "sys.path.append(os.path.split(os.path.split(os.path.split(os.getcwd())[0])[0])[0]) # master drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from tft_torch import tft\n",
    "from typing import Dict,List,Tuple\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if is_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 500\n",
    "n_classes = 4\n",
    "# Prepare sample data\n",
    "timestamp = pd.date_range(start='2020-01-01', periods=n_rows, freq='D')\n",
    "time_series = pd.DataFrame({'values': np.random.randn(n_rows)}, index=timestamp)\n",
    "labels = pd.DataFrame({'label': np.random.randint(0, n_classes, size=n_rows)}, index=timestamp)\n",
    "ext_features = pd.DataFrame({\n",
    "    'feature1': np.random.randn(n_rows),\n",
    "    'feature2': np.random.randn(n_rows)\n",
    "}, index=timestamp)\n",
    "\n",
    "combined_data = time_series.join(labels).join(ext_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 500 entries, 2020-01-01 to 2021-05-14\n",
      "Freq: D\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   values    500 non-null    float64\n",
      " 1   label     500 non-null    int32  \n",
      " 2   feature1  500 non-null    float64\n",
      " 3   feature2  500 non-null    float64\n",
      "dtypes: float64(3), int32(1)\n",
      "memory usage: 33.7 KB\n"
     ]
    }
   ],
   "source": [
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch: TFT Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Test the TemporalFusionTransformer module\"\"\"\n",
    "data_props = {'num_historical_numeric': 2,\n",
    "                'num_historical_categorical': 6,\n",
    "                'num_static_numeric': 10,\n",
    "                'num_static_categorical': 11,\n",
    "                'num_future_numeric': 2,\n",
    "                'num_future_categorical': 3,\n",
    "                'historical_categorical_cardinalities': (1 + np.random.randint(10, size=6)).tolist(), # cardinalities - ie. how many categories each variable has \n",
    "                'static_categorical_cardinalities': (1 + np.random.randint(10, size=11)).tolist(),\n",
    "                'future_categorical_cardinalities': (1 + np.random.randint(10, size=3)).tolist(),\n",
    "                'num_classes': 4\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define batch size and number of historical steps (to model) and future steps (to forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch\n",
    "batch_size = 256\n",
    "historical_steps = 90\n",
    "future_steps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read exogenous features and target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = combined_data.loc[:,['feature1', 'feature2']]\n",
    "label = combined_data['label'][-(len(combined_data)-historical_steps):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch: Data Setup (Static / Future / Observed)\n",
    "\n",
    "`Please note: at the moment we're filling the static covariates with random data, but this may need to be changed in future to adjust for eg. business days / holidays`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tft_train_utils import stack_past_values\n",
    "n_obs = n_rows - historical_steps\n",
    "batch = {\n",
    "        'static_feats_numeric': torch.rand(n_obs, data_props['num_static_numeric'],\n",
    "                                           dtype=torch.float32),\n",
    "        'static_feats_categorical': torch.stack([torch.randint(c, size=(n_obs,)) for c in\n",
    "                                                 data_props['static_categorical_cardinalities']],\n",
    "                                                dim=-1).type(torch.LongTensor),\n",
    "        \n",
    "        'historical_ts_numeric': torch.tensor(stack_past_values(feat.values, historical_steps), dtype=torch.float32),\n",
    "\n",
    "        'historical_ts_categorical': torch.stack([torch.randint(c, size=(n_obs, historical_steps)) for c in\n",
    "                                                  data_props['historical_categorical_cardinalities']],\n",
    "                                                 dim=-1).type(torch.LongTensor),\n",
    "        'future_ts_numeric': torch.rand(n_obs, future_steps, data_props['num_future_numeric'],\n",
    "                                        dtype=torch.float32),\n",
    "        'future_ts_categorical': torch.stack([torch.randint(c, size=(n_obs, future_steps)) for c in\n",
    "                                              data_props['future_categorical_cardinalities']],\n",
    "                                             dim=-1).type(torch.LongTensor),\n",
    "        'target' : torch.reshape(torch.tensor(label[-(n_obs):].values, \n",
    "                        dtype=torch.int64), (n_obs, future_steps))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch - TFT Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TFT: Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "        'model':\n",
    "            {\n",
    "                'dropout': 0.05,\n",
    "                'state_size': 64,\n",
    "                # 'output_quantiles': [0.1, 0.5, 0.9],\n",
    "                'lstm_layers': 2,\n",
    "                'attention_heads': 4\n",
    "            },\n",
    "        'optimization':\n",
    "        {\n",
    "            'batch_size': 256,\n",
    "            'learning_rate': 1e-3,\n",
    "            'max_grad_norm': 1.0\n",
    "        },\n",
    "        # these arguments are related to possible extensions of the model class\n",
    "        'task_type': 'classification',\n",
    "        'target_window_start': None,\n",
    "        'data_props': data_props\n",
    "    }\n",
    "\n",
    "model = tft.TemporalFusionTransformer(OmegaConf.create(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch-Lightning: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tft_pl import TemporalFusionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create instance of TFT Pytorch-Ligthning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalFusionTransformer(config=OmegaConf.create(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compile dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "stat_feat_num = batch['static_feats_numeric']\n",
    "stat_feat_cat = batch['static_feats_categorical']\n",
    "hist_ts_num = batch['historical_ts_numeric']\n",
    "hist_ts_cat = batch['historical_ts_categorical']\n",
    "futr_ts_num = batch['future_ts_numeric']\n",
    "futr_ts_cat = batch['future_ts_categorical']\n",
    "target = batch['target']\n",
    "\n",
    "train_data = TensorDataset(\n",
    "    stat_feat_num, stat_feat_cat, hist_ts_num, hist_ts_cat,\n",
    "    futr_ts_num, futr_ts_cat, target)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run the first small training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                                 | Type                            | Params\n",
      "------------------------------------------------------------------------------------------\n",
      "0  | static_transform                     | InputChannelEmbedding           | 5.7 K \n",
      "1  | historical_ts_transform              | InputChannelEmbedding           | 2.8 K \n",
      "2  | future_ts_transform                  | InputChannelEmbedding           | 1.3 K \n",
      "3  | static_selection                     | VariableSelectionNetwork        | 468 K \n",
      "4  | historical_ts_selection              | VariableSelectionNetwork        | 175 K \n",
      "5  | future_ts_selection                  | VariableSelectionNetwork        | 110 K \n",
      "6  | static_encoder_selection             | GatedResidualNetwork            | 16.8 K\n",
      "7  | static_encoder_enrichment            | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_encoder_sequential_cell_init  | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_encoder_sequential_state_init | GatedResidualNetwork            | 16.8 K\n",
      "10 | past_lstm                            | LSTM                            | 66.6 K\n",
      "11 | future_lstm                          | LSTM                            | 66.6 K\n",
      "12 | post_lstm_gating                     | GateAddNorm                     | 8.4 K \n",
      "13 | static_enrichment_grn                | GatedResidualNetwork            | 20.9 K\n",
      "14 | multihead_attn                       | InterpretableMultiHeadAttention | 41.6 K\n",
      "15 | post_attention_gating                | GateAddNorm                     | 8.4 K \n",
      "16 | pos_wise_ff_grn                      | GatedResidualNetwork            | 16.8 K\n",
      "17 | pos_wise_ff_gating                   | GateAddNorm                     | 8.4 K \n",
      "18 | output_layer                         | Linear                          | 260   \n",
      "------------------------------------------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.280     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 13/13 [00:04<00:00,  2.84it/s, v_num=42, train_loss=1.140]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 13/13 [00:04<00:00,  2.73it/s, v_num=42, train_loss=1.140]\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "from sklearn.metrics import log_loss\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.nn.functional import nll_loss\n",
    "\n",
    "def tft_objective(trial):\n",
    "\n",
    "    vars = ['static_feats_numeric', 'static_feats_categorical',\n",
    "        'historical_ts_numeric', 'historical_ts_categorical',\n",
    "        'future_ts_numeric', 'future_ts_categorical', 'target']\n",
    "\n",
    "    config = configuration\n",
    "    dataset = train_data\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_categorical('lr', [1e-5, 1e-3, 1e-2])\n",
    "    num_heads = trial.suggest_categorical('num_heads', [1, 2, 4])\n",
    "    dropout_prob = trial.suggest_categorical('dropout_prob', [0.1, 0.3, 0.5])\n",
    "    hidden_units = trial.suggest_categorical('hidden_units', [64, 128, 256])\n",
    "    lstm_layers = trial.suggest_categorical('lstm_layers', [1, 2, 4])\n",
    "    # classifier_units = trial.suggest_categorical('classifier_units', [16, 32, 64])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "\n",
    "    config['optimization']['learning_rate'] = lr\n",
    "    config['optimization']['batch_size'] = batch_size\n",
    "    config['model']['dropout'] = dropout_prob\n",
    "    config['model']['state_size'] = hidden_units\n",
    "    config['model']['lstm_layers'] = lstm_layers\n",
    "    config['model']['attention_heads'] = num_heads\n",
    "\n",
    "    # Initialize the model with suggested hyperparameters\n",
    "    model = TemporalFusionTransformer(config=OmegaConf.create(config))\n",
    "\n",
    "    # Time series split\n",
    "    kfold = KFold(n_splits=5, shuffle=False)\n",
    "    cv_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=10,\n",
    "            callbacks=[EarlyStopping(monitor='train_loss', patience=5, mode='min')],\n",
    "            logger=False,\n",
    "            enable_checkpointing=False,\n",
    "            enable_model_summary=False\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.fit(model, train_loader)\n",
    "\n",
    "        # Validate the model\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {v:i for v,i in zip(vars, batch)}\n",
    "                logits = model(batch)['predicted_quantiles']\n",
    "                # preds = torch.argmax(classification, dim=1)\n",
    "                all_preds.extend(logits.squeeze(1).cpu().numpy())\n",
    "                all_targets.extend(batch['target'].flatten().cpu().numpy())\n",
    "\n",
    "        # val_predictions = trainer.predict(model, val_loader)\n",
    "        # val_predictions = torch.cat([x for x in val_predictions], dim=0).numpy()\n",
    "        \n",
    "        val_loss = nll_loss(torch.tensor(all_preds), torch.tensor(all_targets))\n",
    "        cv_scores.append(val_loss)\n",
    "\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run hyperparameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(tft_objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read the best set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration['optimization']['learning_rate'] = study.best_params['lr']\n",
    "configuration['optimization']['batch_size'] = study.best_params['batch_size']\n",
    "configuration['model']['dropout'] = study.best_params['dropout_prob']\n",
    "configuration['model']['state_size'] = study.best_params['hidden_units']\n",
    "configuration['model']['lstm_layers'] = study.best_params['lstm_layers']\n",
    "configuration['model']['attention_heads'] = study.best_params['num_heads']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalFusionTransformer(config=OmegaConf.create(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    callbacks=[EarlyStopping(monitor='train_loss', patience=10, mode='min')]\n",
    "    ) # remember to add the callbacks\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: Perform Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hist_ts_num.detach().numpy()[:,0,:] # take the first 2d input for cv\n",
    "y = target.detach().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/9\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 13.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 24/24 [00:06<00:00,  3.65it/s, v_num=43, train_loss=0.763, val_loss=1.240]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 24/24 [00:06<00:00,  3.47it/s, v_num=43, train_loss=0.763, val_loss=1.240]\n",
      "Cross-Validation results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Label_0       0.71      0.38      0.50        13\n",
      "     Label_1       0.50      0.71      0.59         7\n",
      "     Label_2       0.44      0.80      0.57        10\n",
      "     Label_3       0.50      0.27      0.35        11\n",
      "\n",
      "    accuracy                           0.51        41\n",
      "   macro avg       0.54      0.54      0.50        41\n",
      "weighted avg       0.55      0.51      0.49        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from CPCV.cpcv import CombPurgedKFoldCVLocal\n",
    "from tft_train_utils import cross_validate_model\n",
    "\n",
    "date_index = combined_data[historical_steps:].index\n",
    "\n",
    "pred_times = pd.Series(date_index, index=date_index)\n",
    "eval_times = pd.Series(date_index, index=date_index)\n",
    "\n",
    "# Construct CPCV in-line with DePrado method\n",
    "cpcv = CombPurgedKFoldCVLocal(\n",
    "    n_splits=10,\n",
    "    n_test_splits=1,\n",
    "    embargo_td=pd.Timedelta(days=2)\n",
    ")\n",
    "\n",
    "cv_split = cpcv.split(\n",
    "    pd.DataFrame(X, index=date_index), \n",
    "    pd.Series(y, index=date_index), \n",
    "    pred_times, \n",
    "    eval_times)\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate_model(\n",
    "    train_data,\n",
    "    model=model,\n",
    "    num_epochs=10,\n",
    "    num_classes=data_props['num_classes'],\n",
    "    cv_split=cv_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: MC Dropout Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mc_dropout_tft import mc_dropout\n",
    "mean_predictions, std_predictions = mc_dropout(model, train_dataloader, mc_iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.argmax(mean_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Export the reuslts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Save test predictions to a CSV\n",
    "test_df = pd.DataFrame({\n",
    "    'Prediction': predicted_labels,\n",
    "    'Probability_0': [p[0] for p in mean_predictions],\n",
    "    'Probability_1': [p[1] for p in mean_predictions],\n",
    "    'Probability_2': [p[2] for p in mean_predictions],  # Adjust based on num_classes\n",
    "    'Probability_3': [p[3] for p in mean_predictions],\n",
    "    'Uncertainty_0': [u[0] for u in std_predictions],\n",
    "    'Uncertainty_1': [u[1] for u in std_predictions],\n",
    "    'Uncertainty_2': [u[2] for u in std_predictions],\n",
    "    'Uncertainty_3': [u[3] for u in std_predictions]\n",
    "})\n",
    "\n",
    "test_df.to_csv('tft_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "save(model.state_dict(), 'tft_classifier.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GluonTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

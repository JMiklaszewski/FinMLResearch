{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.path.split(os.getcwd())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from tft_torch import tft\n",
    "from typing import Dict,List,Tuple\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if is_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 500\n",
    "n_classes = 4\n",
    "# Prepare sample data\n",
    "timestamp = pd.date_range(start='2020-01-01', periods=n_rows, freq='D')\n",
    "time_series = pd.DataFrame({'values': np.random.randn(n_rows)}, index=timestamp)\n",
    "labels = pd.DataFrame({'label': np.random.randint(0, n_classes, size=n_rows)}, index=timestamp)\n",
    "ext_features = pd.DataFrame({\n",
    "    'feature1': np.random.randn(n_rows),\n",
    "    'feature2': np.random.randn(n_rows)\n",
    "}, index=timestamp)\n",
    "\n",
    "combined_data = time_series.join(labels).join(ext_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 500 entries, 2020-01-01 to 2021-05-14\n",
      "Freq: D\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   values    500 non-null    float64\n",
      " 1   label     500 non-null    int32  \n",
      " 2   feature1  500 non-null    float64\n",
      " 3   feature2  500 non-null    float64\n",
      "dtypes: float64(3), int32(1)\n",
      "memory usage: 33.7 KB\n"
     ]
    }
   ],
   "source": [
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch: TFT Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Test the TemporalFusionTransformer module\"\"\"\n",
    "data_props = {'num_historical_numeric': 2,\n",
    "                'num_historical_categorical': 6,\n",
    "                'num_static_numeric': 10,\n",
    "                'num_static_categorical': 11,\n",
    "                'num_future_numeric': 2,\n",
    "                'num_future_categorical': 3,\n",
    "                'historical_categorical_cardinalities': (1 + np.random.randint(10, size=6)).tolist(), # cardinalities - ie. how many categories each variable has \n",
    "                'static_categorical_cardinalities': (1 + np.random.randint(10, size=11)).tolist(),\n",
    "                'future_categorical_cardinalities': (1 + np.random.randint(10, size=3)).tolist(),\n",
    "                'num_classes': 4\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define batch size and number of historical steps (to model) and future steps (to forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch\n",
    "batch_size = 256\n",
    "historical_steps = 90\n",
    "future_steps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read exogenous features and target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = combined_data.loc[:,['feature1', 'feature2']]\n",
    "label = combined_data['label'][-(len(combined_data)-historical_steps):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch: Data Setup (Static / Future / Observed)\n",
    "\n",
    "`Please note: at the moment we're filling the static covariates with random data, but this may need to be changed in future to adjust for eg. business days / holidays`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tft_train_utils import stack_past_values\n",
    "n_obs = n_rows - historical_steps\n",
    "batch = {\n",
    "        'static_feats_numeric': torch.rand(n_obs, data_props['num_static_numeric'],\n",
    "                                           dtype=torch.float32),\n",
    "        'static_feats_categorical': torch.stack([torch.randint(c, size=(n_obs,)) for c in\n",
    "                                                 data_props['static_categorical_cardinalities']],\n",
    "                                                dim=-1).type(torch.LongTensor),\n",
    "        \n",
    "        'historical_ts_numeric': torch.tensor(stack_past_values(feat.values, historical_steps), dtype=torch.float32),\n",
    "\n",
    "        'historical_ts_categorical': torch.stack([torch.randint(c, size=(n_obs, historical_steps)) for c in\n",
    "                                                  data_props['historical_categorical_cardinalities']],\n",
    "                                                 dim=-1).type(torch.LongTensor),\n",
    "        'future_ts_numeric': torch.rand(n_obs, future_steps, data_props['num_future_numeric'],\n",
    "                                        dtype=torch.float32),\n",
    "        'future_ts_categorical': torch.stack([torch.randint(c, size=(n_obs, future_steps)) for c in\n",
    "                                              data_props['future_categorical_cardinalities']],\n",
    "                                             dim=-1).type(torch.LongTensor),\n",
    "        'target' : torch.reshape(torch.tensor(label[-(n_obs):].values, \n",
    "                        dtype=torch.int64), (n_obs, future_steps))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch - TFT Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TFT: Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\n",
    "        'model':\n",
    "            {\n",
    "                'dropout': 0.05,\n",
    "                'state_size': 64,\n",
    "                # 'output_quantiles': [0.1, 0.5, 0.9],\n",
    "                'lstm_layers': 2,\n",
    "                'attention_heads': 4\n",
    "            },\n",
    "        'optimization':\n",
    "        {\n",
    "            'batch_size': 256,\n",
    "            'learning_rate': 1e-3,\n",
    "            'max_grad_norm': 1.0\n",
    "        },\n",
    "        # these arguments are related to possible extensions of the model class\n",
    "        'task_type': 'classification',\n",
    "        'target_window_start': None,\n",
    "        'data_props': data_props\n",
    "    }\n",
    "\n",
    "model = tft.TemporalFusionTransformer(OmegaConf.create(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch-Lightning: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tft_pl import TemporalFusionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create instance of TFT Pytorch-Ligthning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalFusionTransformer(config=OmegaConf.create(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compile dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "stat_feat_num = batch['static_feats_numeric']\n",
    "stat_feat_cat = batch['static_feats_categorical']\n",
    "hist_ts_num = batch['historical_ts_numeric']\n",
    "hist_ts_cat = batch['historical_ts_categorical']\n",
    "futr_ts_num = batch['future_ts_numeric']\n",
    "futr_ts_cat = batch['future_ts_categorical']\n",
    "target = batch['target']\n",
    "\n",
    "train_data = TensorDataset(\n",
    "    stat_feat_num, stat_feat_cat, hist_ts_num, hist_ts_cat,\n",
    "    futr_ts_num, futr_ts_cat, target)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run the first small training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                                 | Type                            | Params\n",
      "------------------------------------------------------------------------------------------\n",
      "0  | static_transform                     | InputChannelEmbedding           | 5.6 K \n",
      "1  | historical_ts_transform              | InputChannelEmbedding           | 2.4 K \n",
      "2  | future_ts_transform                  | InputChannelEmbedding           | 1.4 K \n",
      "3  | static_selection                     | VariableSelectionNetwork        | 468 K \n",
      "4  | historical_ts_selection              | VariableSelectionNetwork        | 175 K \n",
      "5  | future_ts_selection                  | VariableSelectionNetwork        | 110 K \n",
      "6  | static_encoder_selection             | GatedResidualNetwork            | 16.8 K\n",
      "7  | static_encoder_enrichment            | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_encoder_sequential_cell_init  | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_encoder_sequential_state_init | GatedResidualNetwork            | 16.8 K\n",
      "10 | past_lstm                            | LSTM                            | 66.6 K\n",
      "11 | future_lstm                          | LSTM                            | 66.6 K\n",
      "12 | post_lstm_gating                     | GateAddNorm                     | 8.4 K \n",
      "13 | static_enrichment_grn                | GatedResidualNetwork            | 20.9 K\n",
      "14 | multihead_attn                       | InterpretableMultiHeadAttention | 41.6 K\n",
      "15 | post_attention_gating                | GateAddNorm                     | 8.4 K \n",
      "16 | pos_wise_ff_grn                      | GatedResidualNetwork            | 16.8 K\n",
      "17 | pos_wise_ff_gating                   | GateAddNorm                     | 8.4 K \n",
      "18 | output_layer                         | Linear                          | 260   \n",
      "------------------------------------------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.278     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 13/13 [00:05<00:00,  2.60it/s, v_num=36, train_loss=-0.531]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 13/13 [00:05<00:00,  2.43it/s, v_num=36, train_loss=-0.531]\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/13 [23:05<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/13 [21:54<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/13 [17:47<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/13 [17:34<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/13 [16:36<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "from sklearn.metrics import log_loss\n",
    "import optuna\n",
    "import numpy as np\n",
    "from cpcv import CombPurgedKFoldCVLocal\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.nn.functional import nll_loss\n",
    "\n",
    "def tft_objective(trial):\n",
    "\n",
    "    vars = ['static_feats_numeric', 'static_feats_categorical',\n",
    "        'historical_ts_numeric', 'historical_ts_categorical',\n",
    "        'future_ts_numeric', 'future_ts_categorical', 'target']\n",
    "\n",
    "    config = configuration\n",
    "    dataset = train_data\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_categorical('lr', [1e-5, 1e-3, 1e-2])\n",
    "    num_heads = trial.suggest_categorical('num_heads', [1, 2, 4])\n",
    "    dropout_prob = trial.suggest_categorical('dropout_prob', [0.1, 0.3, 0.5])\n",
    "    hidden_units = trial.suggest_categorical('hidden_units', [64, 128, 256])\n",
    "    lstm_layers = trial.suggest_categorical('lstm_layers', [1, 2, 4])\n",
    "    # classifier_units = trial.suggest_categorical('classifier_units', [16, 32, 64])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "\n",
    "    config['optimization']['learning_rate'] = lr\n",
    "    config['optimization']['batch_size'] = batch_size\n",
    "    config['model']['dropout'] = dropout_prob\n",
    "    config['model']['state_size'] = hidden_units\n",
    "    config['model']['lstm_layers'] = lstm_layers\n",
    "    config['model']['attention_heads'] = num_heads\n",
    "\n",
    "    # Initialize the model with suggested hyperparameters\n",
    "    model = TemporalFusionTransformer(config=OmegaConf.create(config))\n",
    "\n",
    "    pred_times = pd.Series(df.index, index=df.index)\n",
    "    eval_times = pd.Series(df.index, index=df.index)\n",
    "    \n",
    "    # Time series split\n",
    "    cpcv = CombPurgedKFoldCVLocal(\n",
    "        n_splits=10,\n",
    "        n_test_splits=1,\n",
    "        embargo_td=pd.Timedelta(days=2)\n",
    "        )\n",
    "    cv_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cpcv.split(X, y, pred_times, eval_times)):\n",
    "\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=10,\n",
    "            callbacks=[EarlyStopping(monitor='train_loss', patience=5, mode='min')],\n",
    "            logger=False,\n",
    "            enable_checkpointing=False,\n",
    "            enable_model_summary=False\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.fit(model, train_loader)\n",
    "\n",
    "        # Validate the model\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {v:i for v,i in zip(vars, batch)}\n",
    "                logits = model(batch)['predicted_quantiles']\n",
    "                # preds = torch.argmax(classification, dim=1)\n",
    "                all_preds.extend(logits.squeeze(1).cpu().numpy())\n",
    "                all_targets.extend(batch['target'].flatten().cpu().numpy())\n",
    "\n",
    "        # val_predictions = trainer.predict(model, val_loader)\n",
    "        # val_predictions = torch.cat([x for x in val_predictions], dim=0).numpy()\n",
    "        \n",
    "        val_loss = nll_loss(torch.tensor(all_preds), torch.tensor(all_targets))\n",
    "        cv_scores.append(val_loss)\n",
    "\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run hyperparameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-07 17:10:33,709] A new study created in memory with name: no-name-00c2884e-6017-49b6-81df-e26961175d50\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  1.97s/it, train_loss=-0.317]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  1.97s/it, train_loss=-0.317]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it, train_loss=-0.418]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it, train_loss=-0.418]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it, train_loss=-0.488]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it, train_loss=-0.488]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, train_loss=-0.572]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  1.98s/it, train_loss=-0.639]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  1.99s/it, train_loss=-0.639]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-07 18:41:04,323] Trial 0 finished with value: -0.42560768127441406 and parameters: {'lr': 0.001, 'num_heads': 1, 'dropout_prob': 0.5, 'hidden_units': 64, 'embed_dim': 2, 'batch_size': 256}. Best is trial 0 with value: -0.42560768127441406.\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 7/7 [00:05<00:00,  1.30it/s, train_loss=-0.333] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 7/7 [00:05<00:00,  1.30it/s, train_loss=-0.333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 7/7 [00:06<00:00,  1.09it/s, train_loss=-0.308] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 7/7 [00:08<00:00,  1.20s/it, train_loss=-0.308]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 7/7 [00:06<00:00,  1.01it/s, train_loss=-0.308]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 7/7 [00:07<00:00,  1.06s/it, train_loss=-0.154]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-07 18:45:01,562] Trial 1 finished with value: -0.2969079613685608 and parameters: {'lr': 0.01, 'num_heads': 1, 'dropout_prob': 0.1, 'hidden_units': 128, 'embed_dim': 1, 'batch_size': 32}. Best is trial 0 with value: -0.42560768127441406.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:12<00:00, 12.73s/it, train_loss=-0.544]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:12<00:00, 12.74s/it, train_loss=-0.544]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:11<00:00, 11.58s/it, train_loss=-0.571]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:11<00:00, 11.59s/it, train_loss=-0.571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:12<00:00, 12.03s/it, train_loss=-0.594]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:12<00:00, 12.03s/it, train_loss=-0.594]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:12<00:00, 12.90s/it, train_loss=-0.638]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:12<00:00, 12.90s/it, train_loss=-0.638]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:11<00:00, 11.01s/it, train_loss=-0.707]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:11<00:00, 11.02s/it, train_loss=-0.707]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-07 18:55:22,748] Trial 2 finished with value: -0.4813499450683594 and parameters: {'lr': 0.001, 'num_heads': 1, 'dropout_prob': 0.3, 'hidden_units': 256, 'embed_dim': 4, 'batch_size': 256}. Best is trial 2 with value: -0.4813499450683594.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  1.99s/it, train_loss=-0.559]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  1.99s/it, train_loss=-0.559]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it, train_loss=-0.467]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it, train_loss=-0.56] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it, train_loss=-0.56]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1/1 [00:02<00:00,  2.44s/it, train_loss=-0.551]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it, train_loss=-0.651]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it, train_loss=-0.651]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-07 18:56:52,147] Trial 3 finished with value: -0.3943663239479065 and parameters: {'lr': 0.01, 'num_heads': 4, 'dropout_prob': 0.1, 'hidden_units': 64, 'embed_dim': 1, 'batch_size': 256}. Best is trial 2 with value: -0.4813499450683594.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it, train_loss=-0.254]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it, train_loss=-0.261]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it, train_loss=-0.261]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it, train_loss=-0.267]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it, train_loss=-0.267]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it, train_loss=-0.27] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it, train_loss=-0.253]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-07 18:58:39,659] Trial 4 finished with value: -0.26660317182540894 and parameters: {'lr': 1e-05, 'num_heads': 4, 'dropout_prob': 0.3, 'hidden_units': 64, 'embed_dim': 4, 'batch_size': 128}. Best is trial 2 with value: -0.4813499450683594.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(tft_objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read the best set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Print best hyperparameters\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mstudy\u001b[49m\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lstm_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m configuration[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_prob\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m configuration[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_units\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m configuration[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_layers\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlstm_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m configuration[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_heads\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_heads\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lstm_layers'"
     ]
    }
   ],
   "source": [
    "configuration['optimization']['learning_rate'] = study.best_params['lr']\n",
    "configuration['optimization']['batch_size'] = study.best_params['batch_size']\n",
    "configuration['model']['dropout'] = study.best_params['dropout_prob']\n",
    "configuration['model']['state_size'] = study.best_params['hidden_units']\n",
    "configuration['model']['lstm_layers'] = study.best_params['lstm_layers']\n",
    "configuration['model']['attention_heads'] = study.best_params['num_heads']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalFusionTransformer(config=OmegaConf.create(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                                 | Type                            | Params\n",
      "------------------------------------------------------------------------------------------\n",
      "0  | static_transform                     | InputChannelEmbedding           | 5.6 K \n",
      "1  | historical_ts_transform              | InputChannelEmbedding           | 2.4 K \n",
      "2  | future_ts_transform                  | InputChannelEmbedding           | 1.4 K \n",
      "3  | static_selection                     | VariableSelectionNetwork        | 468 K \n",
      "4  | historical_ts_selection              | VariableSelectionNetwork        | 175 K \n",
      "5  | future_ts_selection                  | VariableSelectionNetwork        | 110 K \n",
      "6  | static_encoder_selection             | GatedResidualNetwork            | 16.8 K\n",
      "7  | static_encoder_enrichment            | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_encoder_sequential_cell_init  | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_encoder_sequential_state_init | GatedResidualNetwork            | 16.8 K\n",
      "10 | past_lstm                            | LSTM                            | 66.6 K\n",
      "11 | future_lstm                          | LSTM                            | 66.6 K\n",
      "12 | post_lstm_gating                     | GateAddNorm                     | 8.4 K \n",
      "13 | static_enrichment_grn                | GatedResidualNetwork            | 20.9 K\n",
      "14 | multihead_attn                       | InterpretableMultiHeadAttention | 41.6 K\n",
      "15 | post_attention_gating                | GateAddNorm                     | 8.4 K \n",
      "16 | pos_wise_ff_grn                      | GatedResidualNetwork            | 16.8 K\n",
      "17 | pos_wise_ff_gating                   | GateAddNorm                     | 8.4 K \n",
      "18 | output_layer                         | Linear                          | 260   \n",
      "------------------------------------------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.278     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 13/13 [00:05<00:00,  2.42it/s, v_num=37, train_loss=-0.615]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 13/13 [00:05<00:00,  2.26it/s, v_num=37, train_loss=-0.615]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    callbacks=[EarlyStopping(monitor='train_loss', patience=10, mode='min')]\n",
    "    ) # remember to add the callbacks\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: Perform Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 13/13 [00:04<00:00,  2.64it/s, v_num=25, train_loss=-0.65, val_loss=-0.493] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 13/13 [00:05<00:00,  2.39it/s, v_num=25, train_loss=-0.65, val_loss=-0.493]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5\n",
      "Epoch 9: 100%|██████████| 13/13 [00:05<00:00,  2.53it/s, v_num=26, train_loss=-0.689, val_loss=-0.608]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 13/13 [00:05<00:00,  2.42it/s, v_num=26, train_loss=-0.689, val_loss=-0.608]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5\n",
      "Epoch 9: 100%|██████████| 13/13 [00:05<00:00,  2.17it/s, v_num=27, train_loss=-0.714, val_loss=-0.715]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 13/13 [00:06<00:00,  2.06it/s, v_num=27, train_loss=-0.714, val_loss=-0.715]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4/5\n",
      "Epoch 9: 100%|██████████| 13/13 [00:06<00:00,  2.15it/s, v_num=28, train_loss=-0.763, val_loss=-0.685]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 13/13 [00:06<00:00,  1.97it/s, v_num=28, train_loss=-0.763, val_loss=-0.685]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5/5\n",
      "Epoch 9: 100%|██████████| 13/13 [00:04<00:00,  2.67it/s, v_num=29, train_loss=-0.692, val_loss=-0.687]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 13/13 [00:05<00:00,  2.55it/s, v_num=29, train_loss=-0.692, val_loss=-0.687]\n",
      "Cross-Validation results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Label_0       0.60      0.56      0.58        61\n",
      "     Label_1       0.69      0.63      0.66        76\n",
      "     Label_2       0.65      0.67      0.66        58\n",
      "     Label_3       0.61      0.69      0.65        61\n",
      "\n",
      "    accuracy                           0.64       256\n",
      "   macro avg       0.64      0.64      0.64       256\n",
      "weighted avg       0.64      0.64      0.64       256\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'train_loss': -0.6499637365341187, 'val_loss': -0.4925090968608856},\n",
       " {'train_loss': -0.6885847449302673, 'val_loss': -0.6076631546020508},\n",
       " {'train_loss': -0.7136156558990479, 'val_loss': -0.7154910564422607},\n",
       " {'train_loss': -0.7629744410514832, 'val_loss': -0.6847378611564636},\n",
       " {'train_loss': -0.6921852827072144, 'val_loss': -0.6872054934501648}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tft_train_utils import cross_validate_model\n",
    "\n",
    "cv_results = cross_validate_model(\n",
    "    train_data, \n",
    "    num_splits=5, \n",
    "    model=model, \n",
    "    num_epochs=10, \n",
    "    num_classes=data_props['num_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: MC Dropout Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [12:02<00:00,  7.22s/it]\n"
     ]
    }
   ],
   "source": [
    "from tft_train_utils import mc_dropout_predictions \n",
    "# Perform MC Dropout predictions\n",
    "mc_predictions = mc_dropout_predictions(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predictions = mc_predictions.mean(axis=0).squeeze()\n",
    "std_predictions = mc_predictions.std(axis=0).squeeze()\n",
    "predicted_labels = np.argmax(mean_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Export the reuslts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Save test predictions to a CSV\n",
    "test_df = pd.DataFrame({\n",
    "    'Prediction': predicted_labels,\n",
    "    'Probability_0': [p[0] for p in mean_predictions],\n",
    "    'Probability_1': [p[1] for p in mean_predictions],\n",
    "    'Probability_2': [p[2] for p in mean_predictions],  # Adjust based on num_classes\n",
    "    'Probability_3': [p[3] for p in mean_predictions],\n",
    "    'Uncertainty_0': [u[0] for u in std_predictions],\n",
    "    'Uncertainty_1': [u[1] for u in std_predictions],\n",
    "    'Uncertainty_2': [u[2] for u in std_predictions],\n",
    "    'Uncertainty_3': [u[3] for u in std_predictions]\n",
    "})\n",
    "\n",
    "test_df.to_csv('tft_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch-Lightning: Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "save(model.state_dict(), 'tft_classifier.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GluonTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

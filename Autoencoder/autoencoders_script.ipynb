{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "cur_dir = os.getcwd()\n",
    "# Add the current directory to system path (including parent folder)\n",
    "sys.path.append(cur_dir)\n",
    "sys.path.append(os.path.split(cur_dir)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Prepare sample data\n",
    "timestamp = pd.date_range(start='2020-01-01', periods=300, freq='D')\n",
    "time_series = pd.DataFrame({'values': np.random.randn(300)}, index=timestamp)\n",
    "labels = pd.DataFrame({'label': np.random.randint(0, 3, size=300)}, index=timestamp)\n",
    "ext_features = pd.DataFrame({\n",
    "    'feature1': np.random.randn(300),\n",
    "    'feature2': np.random.randn(300)\n",
    "}, index=timestamp)\n",
    "\n",
    "combined_data = time_series.join(labels).join(ext_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Validation / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Validation / Test Split\n",
    "train_data = combined_data[:100]\n",
    "val_data = combined_data[100:200]\n",
    "test_data = combined_data[200:]\n",
    "\n",
    "# Read targets\n",
    "train_target = train_data.label.values\n",
    "val_target = val_data.label.values\n",
    "test_target = test_data.label.values\n",
    "\n",
    "# Read features\n",
    "train_features = train_data[['feature1', 'feature2']].values\n",
    "val_features = val_data[['feature1', 'feature2']].values\n",
    "test_features = test_data[['feature1', 'feature2']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pl_model_utils import TimeSeriesDataModule\n",
    "# Instantiate data module and model\n",
    "data_module = TimeSeriesDataModule(\n",
    "    train_target, train_features,\n",
    "    val_target, val_features,\n",
    "    test_target, test_features,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Setup the data for model\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Basic Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compile simple Autoencoder for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_autoencoder_classifiers import AutoencoderClassifier\n",
    "ae_model = AutoencoderClassifier(context_length=1, num_classes=3, num_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='train_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00, 102.87it/s, v_num=93]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00, 87.55it/s, v_num=93] \n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "# Train the model\n",
    "trainer = Trainer(max_epochs=20, callbacks=[checkpoint_callback])\n",
    "trainer.fit(ae_model, data_module.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross-Validation (Using Combinatorial Purged K-Fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 67.44it/s, v_num=94] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 57.01it/s, v_num=94]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.51s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 79.86it/s, v_num=95] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 68.73it/s, v_num=95]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:09,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 79.72it/s, v_num=96] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 66.12it/s, v_num=96]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:14,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 90.26it/s, v_num=97] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 75.57it/s, v_num=97]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:18,  4.56s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 103.46it/s, v_num=98]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 85.87it/s, v_num=98] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:21,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 102.53it/s, v_num=99]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 86.01it/s, v_num=99] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:25,  3.99s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 106.33it/s, v_num=100]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 88.46it/s, v_num=100] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:29,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 105.02it/s, v_num=101]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 88.75it/s, v_num=101] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:33,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 82.27it/s, v_num=102] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 70.42it/s, v_num=102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:37,  4.01s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 94.69it/s, v_num=103] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 80.44it/s, v_num=103]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:41,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:41,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Classification Report:\n",
      "           0.0  1.0  2.0  accuracy  macro avg  weighted avg\n",
      "precision  1.0  1.0  1.0       1.0        1.0           1.0\n",
      "recall     1.0  1.0  1.0       1.0        1.0           1.0\n",
      "f1-score   1.0  1.0  1.0       1.0        1.0           1.0\n",
      "support    3.6  3.7  2.7       1.0       10.0          10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pl_model_utils import cross_validate_model\n",
    "from CPCV.cpcv import CombPurgedKFoldCVLocal\n",
    "\n",
    "\n",
    "pred_times = pd.Series(val_data.index, index=val_data.index)\n",
    "eval_times = pd.Series(val_data.index, index=val_data.index)\n",
    "\n",
    "# Construct CPCV in-line with DePrado method\n",
    "cpcv = CombPurgedKFoldCVLocal(\n",
    "    n_splits=10,\n",
    "    n_test_splits=1,\n",
    "    embargo_td=pd.Timedelta(days=2)\n",
    ")\n",
    "\n",
    "cv_split = cpcv.split(\n",
    "    pd.DataFrame(data_module.val_features, index=val_data.index), \n",
    "    pd.Series(data_module.val_target, index=val_data.index), \n",
    "    pred_times, \n",
    "    eval_times)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate_model(\n",
    "    X=data_module.val_features,\n",
    "    y=data_module.val_target,\n",
    "    model=ae_model,\n",
    "    cv_split=cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 11.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from mc_dropout import mc_dropout # final, one-line function to implement mc dropout (As suggested here: https://github.com/Lightning-AI/pytorch-lightning/blob/f35e2210e240b443fd4dafed8fe2e30ee7d579ea/docs/source/common/production_inference.rst#prediction-api)\n",
    "predictions_mean, predictions_std = mc_dropout(ae_model, data_module.val_dataloader(), mc_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 1, 2, 0, 2, 2, 1, 0, 0, 1, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0,\n",
       "       0, 0, 2, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 1, 2, 0, 0,\n",
       "       0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1, 0, 1, 0, 2, 0, 0, 2, 2, 1, 1, 0,\n",
       "       2, 2, 2, 1, 0, 1, 2, 1, 2, 0, 0, 1, 1, 0, 0, 1, 2, 1, 0, 2, 2, 0,\n",
       "       2, 1, 0, 2, 2, 2, 1, 2, 1, 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions_mean, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Attention-Enchanced Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pl_autoencoder_classifiers import AutoencoderAttentionClassifier\n",
    "from pl_model_utils import TimeSeriesDataset\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import log_loss\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "def ae_attention_objective(trial):\n",
    "    context_length = 1\n",
    "    num_classes = 3\n",
    "    num_features = 2\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_categorical('lr', [1e-5, 1e-3, 1e-2])\n",
    "    num_heads = trial.suggest_categorical('num_heads', [1, 2, 4])\n",
    "    dropout_prob = trial.suggest_categorical('dropout_prob', [0.1, 0.3, 0.5])\n",
    "    hidden_units = trial.suggest_categorical('hidden_units', [64, 128, 256])\n",
    "    embed_dim = trial.suggest_categorical('embed_dim', [32, 64, 128])\n",
    "    classifier_units = trial.suggest_categorical('classifier_units', [16, 32, 64])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "\n",
    "    # Initialize the model with suggested hyperparameters\n",
    "    model = AutoencoderAttentionClassifier(\n",
    "        context_length=context_length,\n",
    "        num_classes=num_classes,\n",
    "        num_features=num_features,\n",
    "        num_heads=num_heads,\n",
    "        dropout_prob=dropout_prob,\n",
    "        hidden_units=hidden_units,\n",
    "        embed_dim=embed_dim,\n",
    "        classifier_units=classifier_units,\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    # Assuming you have your dataset in `X` and `y`\n",
    "    X, y = data_module.val_features, data_module.val_target\n",
    "    # X = np.array(X_train)  # Ensure X_train is a NumPy array\n",
    "    # y = np.array(y_train)  # Ensure y_train is a NumPy array\n",
    "\n",
    "    # Time series split\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    cv_scores = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        # Create DataLoader for the training and validation fold\n",
    "        train_dataset = TimeSeriesDataset(\n",
    "            torch.tensor(y_train_fold, dtype=torch.float32), \n",
    "            torch.tensor(X_train_fold, dtype=torch.float32)\n",
    "            )\n",
    "        \n",
    "        val_dataset = TimeSeriesDataset(\n",
    "            torch.tensor(y_val_fold, dtype=torch.float32), \n",
    "            torch.tensor(X_val_fold, dtype=torch.float32)\n",
    "            )\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=10,\n",
    "            callbacks=[EarlyStopping(monitor='train_loss', patience=3, mode='min')],\n",
    "            logger=False,\n",
    "            enable_checkpointing=False\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.fit(model, train_loader)\n",
    "\n",
    "        # Validate the model\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                targets, features = batch\n",
    "                _, classification = model(targets, features)\n",
    "                # preds = torch.argmax(classification, dim=1)\n",
    "                all_preds.extend(classification.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        # val_predictions = trainer.predict(model, val_loader)\n",
    "        # val_predictions = torch.cat([x for x in val_predictions], dim=0).numpy()\n",
    "        \n",
    "        val_loss = log_loss(all_targets, all_preds)\n",
    "        cv_scores.append(val_loss)\n",
    "\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(ae_attention_objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_attention_model = AutoencoderAttentionClassifier(\n",
    "    context_length=1, \n",
    "    num_classes=3, \n",
    "    num_features=2,\n",
    "    lr=study.best_params['lr'],\n",
    "    num_heads=study.best_params['num_heads'],\n",
    "    dropout_prob=study.best_params['dropout_prob'],\n",
    "    hidden_units=study.best_params['hidden_units'],\n",
    "    embed_dim=study.best_params['embed_dim'],\n",
    "    classifier_units=study.best_params['classifier_units']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_attention_model = AutoencoderAttentionClassifier(\n",
    "    context_length=1, \n",
    "    num_classes=3, \n",
    "    num_features=2,\n",
    "    lr=0.001,\n",
    "    num_heads=1,\n",
    "    dropout_prob=0.1,\n",
    "    hidden_units=256,\n",
    "    embed_dim=128,\n",
    "    classifier_units=64\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:612: UserWarning: Checkpoint directory c:\\Users\\jedre\\OneDrive\\Pulpit\\State_Street\\Programs\\Python\\ML_Research_Project_2024\\Autoencoder\\lightning_logs\\version_93\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 7/7 [00:00<00:00, 65.29it/s, v_num=104]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 7/7 [00:00<00:00, 55.69it/s, v_num=104]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer = Trainer(max_epochs=10, callbacks=[checkpoint_callback])\n",
    "trainer.fit(ae_attention_model, data_module.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 33.99it/s, v_num=105]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 27.38it/s, v_num=105]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:09,  9.21s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 54.30it/s, v_num=106]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 43.75it/s, v_num=106]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:16,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 52.19it/s, v_num=107]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 42.55it/s, v_num=107]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:24,  7.88s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 48.49it/s, v_num=108]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 39.59it/s, v_num=108]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:31,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 47.22it/s, v_num=109]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 39.62it/s, v_num=109]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:38,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 18.23it/s, v_num=110]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 16.80it/s, v_num=110]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:46,  7.45s/it]GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 55.08it/s, v_num=111]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 45.12it/s, v_num=111]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:53,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 52.37it/s, v_num=112]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 42.12it/s, v_num=112]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:59,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 50.78it/s, v_num=113]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 42.92it/s, v_num=113]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "9it [01:07,  7.19s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 55.57it/s, v_num=114]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 45.10it/s, v_num=114]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:13,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:13,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Classification Report:\n",
      "           0.0       1.0  2.0  accuracy  macro avg  weighted avg\n",
      "precision  1.0  0.980000  0.9      0.99   0.960000      0.982000\n",
      "recall     1.0  1.000000  0.9      0.99   0.966667      0.990000\n",
      "f1-score   1.0  0.988889  0.9      0.99   0.962963      0.985556\n",
      "support    3.6  3.700000  2.7      0.99  10.000000     10.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct CPCV in-line with DePrado method\n",
    "cpcv = CombPurgedKFoldCVLocal(\n",
    "    n_splits=10,\n",
    "    n_test_splits=1,\n",
    "    embargo_td=pd.Timedelta(days=2)\n",
    ")\n",
    "\n",
    "cv_split = cpcv.split(\n",
    "    pd.DataFrame(data_module.val_features, index=val_data.index), \n",
    "    pd.Series(data_module.val_target, index=val_data.index), \n",
    "    pred_times, \n",
    "    eval_times)\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate_model(\n",
    "    X=data_module.val_features, \n",
    "    y=data_module.val_target, \n",
    "    model=ae_attention_model,\n",
    "    cv_split=cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0.0       1.0  2.0  accuracy  macro avg  weighted avg\n",
      "precision  1.0  0.980000  0.9      0.99   0.960000      0.982000\n",
      "recall     1.0  1.000000  0.9      0.99   0.966667      0.990000\n",
      "f1-score   1.0  0.988889  0.9      0.99   0.962963      0.985556\n",
      "support    3.6  3.700000  2.7      0.99  10.000000     10.000000\n"
     ]
    }
   ],
   "source": [
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  3.77it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_mean, predictions_std = mc_dropout(ae_attention_model, data_module.val_dataloader(), mc_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.argmax(predictions_mean, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Predicted Label = 1, Probabilities = [0.03628777 0.9482291  0.01548309], Uncertainty (std) = [0.10941979 0.15611643 0.04675435]\n",
      "Sample 1: Predicted Label = 1, Probabilities = [0.05393438 0.9226053  0.0234603 ], Uncertainty (std) = [0.12905784 0.1852064  0.0562723 ]\n",
      "Sample 2: Predicted Label = 2, Probabilities = [0.03300069 0.04342121 0.923578  ], Uncertainty (std) = [0.10550195 0.13881221 0.24423368]\n",
      "Sample 3: Predicted Label = 1, Probabilities = [0.0360552  0.94780135 0.01614348], Uncertainty (std) = [0.10876472 0.15743658 0.04884774]\n",
      "Sample 4: Predicted Label = 2, Probabilities = [0.03975081 0.05318822 0.907061  ], Uncertainty (std) = [0.11368055 0.15210102 0.26571855]\n",
      "Sample 5: Predicted Label = 0, Probabilities = [0.9421     0.04370263 0.01419731], Uncertainty (std) = [0.18504454 0.13968307 0.04548705]\n",
      "Sample 6: Predicted Label = 2, Probabilities = [0.02847718 0.03841257 0.9331102 ], Uncertainty (std) = [0.09709278 0.13094921 0.22801423]\n",
      "Sample 7: Predicted Label = 2, Probabilities = [0.04319283 0.05799684 0.89881027], Uncertainty (std) = [0.11757083 0.15792021 0.27543038]\n",
      "Sample 8: Predicted Label = 1, Probabilities = [0.03250013 0.9533334  0.01416651], Uncertainty (std) = [0.10389188 0.14916253 0.04546888]\n",
      "Sample 9: Predicted Label = 0, Probabilities = [0.91691244 0.06241338 0.02067419], Uncertainty (std) = [0.21605988 0.1623138  0.05409691]\n",
      "Sample 10: Predicted Label = 0, Probabilities = [0.9362014  0.0476589  0.01613961], Uncertainty (std) = [0.19237418 0.14374015 0.04879091]\n",
      "Sample 11: Predicted Label = 1, Probabilities = [0.01797312 0.9644462  0.01758073], Uncertainty (std) = [0.07874782 0.14836542 0.10464135]\n",
      "Sample 12: Predicted Label = 1, Probabilities = [0.05041698 0.9271562  0.02242679], Uncertainty (std) = [0.12562624 0.18149637 0.05596485]\n",
      "Sample 13: Predicted Label = 1, Probabilities = [0.03179085 0.95315933 0.01504986], Uncertainty (std) = [0.10161737 0.14970617 0.04821328]\n",
      "Sample 14: Predicted Label = 1, Probabilities = [0.01801147 0.9738116  0.00817689], Uncertainty (std) = [0.07891978 0.11475859 0.03599478]\n",
      "Sample 15: Predicted Label = 0, Probabilities = [0.9359163  0.04824158 0.01584212], Uncertainty (std) = [0.19323067 0.14546959 0.04786921]\n",
      "Sample 16: Predicted Label = 2, Probabilities = [0.03242917 0.04327624 0.9242946 ], Uncertainty (std) = [0.10366596 0.13832715 0.24195758]\n",
      "Sample 17: Predicted Label = 0, Probabilities = [0.9552571  0.03376883 0.01097405], Uncertainty (std) = [0.16392066 0.12372991 0.04025701]\n",
      "Sample 18: Predicted Label = 0, Probabilities = [0.916521   0.06278181 0.02069722], Uncertainty (std) = [0.21705365 0.1632712  0.05393533]\n",
      "Sample 19: Predicted Label = 0, Probabilities = [0.9291335  0.0533606  0.01750591], Uncertainty (std) = [0.20261845 0.15258594 0.05015688]\n",
      "Sample 20: Predicted Label = 0, Probabilities = [0.9290938  0.05324738 0.01765882], Uncertainty (std) = [0.20273788 0.15225822 0.05068185]\n",
      "Sample 21: Predicted Label = 0, Probabilities = [0.92965484 0.05294205 0.01740311], Uncertainty (std) = [0.20112006 0.1513753  0.04988397]\n",
      "Sample 22: Predicted Label = 0, Probabilities = [0.8842775  0.08673023 0.0289923 ], Uncertainty (std) = [0.24827538 0.18608713 0.06240357]\n",
      "Sample 23: Predicted Label = 0, Probabilities = [0.90308625 0.07212517 0.02478863], Uncertainty (std) = [0.23190527 0.17257842 0.05963532]\n",
      "Sample 24: Predicted Label = 2, Probabilities = [0.05758119 0.07684845 0.86557037], Uncertainty (std) = [0.13263263 0.17700769 0.30960286]\n",
      "Sample 25: Predicted Label = 1, Probabilities = [0.03981904 0.9431749  0.01700604], Uncertainty (std) = [0.11386329 0.16246612 0.04871013]\n",
      "Sample 26: Predicted Label = 1, Probabilities = [0.01088443 0.9039906  0.08512489], Uncertainty (std) = [0.06222046 0.28293788 0.27252766]\n",
      "Sample 27: Predicted Label = 1, Probabilities = [0.01455307 0.97924286 0.00620408], Uncertainty (std) = [0.07166181 0.10220387 0.03058123]\n",
      "Sample 28: Predicted Label = 1, Probabilities = [0.03587186 0.9481156  0.0160125 ], Uncertainty (std) = [0.10820134 0.15645391 0.04842558]\n",
      "Sample 29: Predicted Label = 0, Probabilities = [0.9553207  0.03354666 0.01113259], Uncertainty (std) = [0.16368653 0.12296583 0.04091987]\n",
      "Sample 30: Predicted Label = 1, Probabilities = [0.02174373 0.9687498  0.00950643], Uncertainty (std) = [0.08651017 0.12433241 0.03789219]\n",
      "Sample 31: Predicted Label = 0, Probabilities = [0.9300527  0.05255866 0.01738865], Uncertainty (std) = [0.19997849 0.15029283 0.04981282]\n",
      "Sample 32: Predicted Label = 0, Probabilities = [0.92996293 0.05292181 0.01711528], Uncertainty (std) = [0.20023717 0.15134205 0.04898865]\n",
      "Sample 33: Predicted Label = 0, Probabilities = [0.94240355 0.04346849 0.01412794], Uncertainty (std) = [0.18410525 0.1389449  0.04529363]\n",
      "Sample 34: Predicted Label = 1, Probabilities = [0.02219411 0.9683104  0.00949548], Uncertainty (std) = [0.08834381 0.12612797 0.03792311]\n",
      "Sample 35: Predicted Label = 0, Probabilities = [0.9105254  0.06782319 0.02165151], Uncertainty (std) = [0.22289735 0.16897778 0.05403187]\n",
      "Sample 36: Predicted Label = 1, Probabilities = [0.03280954 0.9528951  0.01429526], Uncertainty (std) = [0.10487259 0.15057284 0.04581915]\n",
      "Sample 37: Predicted Label = 2, Probabilities = [0.02161712 0.02932135 0.9490615 ], Uncertainty (std) = [0.08599516 0.11666508 0.20264587]\n",
      "Sample 38: Predicted Label = 1, Probabilities = [0.03954002 0.9433504  0.01710958], Uncertainty (std) = [0.11307012 0.1619729  0.04898489]\n",
      "Sample 39: Predicted Label = 0, Probabilities = [0.9483915  0.03839239 0.01321607], Uncertainty (std) = [0.17591833 0.13088545 0.04514658]\n",
      "Sample 40: Predicted Label = 1, Probabilities = [0.03220307 0.9539307  0.01386623], Uncertainty (std) = [0.10295538 0.14724617 0.0444216 ]\n",
      "Sample 41: Predicted Label = 2, Probabilities = [0.0250545  0.03413752 0.94080794], Uncertainty (std) = [0.09183836 0.12506627 0.21687494]\n",
      "Sample 42: Predicted Label = 0, Probabilities = [0.93599486 0.0476599  0.01634516], Uncertainty (std) = [0.19300415 0.143782   0.04957709]\n",
      "Sample 43: Predicted Label = 0, Probabilities = [0.93567145 0.04815339 0.01617513], Uncertainty (std) = [0.19397086 0.14522415 0.04892065]\n",
      "Sample 44: Predicted Label = 0, Probabilities = [0.9235461  0.05819175 0.01826218], Uncertainty (std) = [0.20809948 0.15839753 0.04978086]\n",
      "Sample 45: Predicted Label = 2, Probabilities = [0.03220211 0.04387558 0.92392236], Uncertainty (std) = [0.10292951 0.14022833 0.24314566]\n",
      "Sample 46: Predicted Label = 0, Probabilities = [0.9301808  0.05233867 0.01748048], Uncertainty (std) = [0.19962905 0.14967513 0.05005462]\n",
      "Sample 47: Predicted Label = 0, Probabilities = [0.97439927 0.01893946 0.00666126], Uncertainty (std) = [0.12605903 0.09326009 0.03280485]\n",
      "Sample 48: Predicted Label = 1, Probabilities = [0.02845551 0.9586613  0.0128831 ], Uncertainty (std) = [0.09700453 0.14090991 0.04402694]\n",
      "Sample 49: Predicted Label = 1, Probabilities = [0.03245686 0.9530992  0.0144439 ], Uncertainty (std) = [0.10376416 0.14991975 0.04630924]\n",
      "Sample 50: Predicted Label = 1, Probabilities = [0.01810095 0.9345357  0.04736336], Uncertainty (std) = [0.0793044  0.22169106 0.19793458]\n",
      "Sample 51: Predicted Label = 1, Probabilities = [0.0429483  0.9378905  0.01916118], Uncertainty (std) = [0.1169207  0.16908343 0.05231015]\n",
      "Sample 52: Predicted Label = 2, Probabilities = [0.03897777 0.05265882 0.90836334], Uncertainty (std) = [0.11145903 0.1505596  0.26200232]\n",
      "Sample 53: Predicted Label = 0, Probabilities = [0.92360395 0.0577633  0.01863273], Uncertainty (std) = [0.20794815 0.15724228 0.05076551]\n",
      "Sample 54: Predicted Label = 1, Probabilities = [0.04700772 0.93189406 0.02109827], Uncertainty (std) = [0.12229701 0.17714235 0.05499587]\n",
      "Sample 55: Predicted Label = 0, Probabilities = [0.9492683 0.0385732 0.0121586], Uncertainty (std) = [0.17291543 0.13151976 0.0415006 ]\n",
      "Sample 56: Predicted Label = 1, Probabilities = [0.04997443 0.9280246  0.02200102], Uncertainty (std) = [0.1245264  0.17930266 0.05490084]\n",
      "Sample 57: Predicted Label = 0, Probabilities = [0.910504   0.06744147 0.0220546 ], Uncertainty (std) = [0.22295192 0.1680473  0.05504065]\n",
      "Sample 58: Predicted Label = 2, Probabilities = [0.04674321 0.06348044 0.8897764 ], Uncertainty (std) = [0.12154828 0.16506365 0.2865929 ]\n",
      "Sample 59: Predicted Label = 0, Probabilities = [0.93593377 0.04850982 0.0155564 ], Uncertainty (std) = [0.19318673 0.1462887  0.04701595]\n",
      "Sample 60: Predicted Label = 0, Probabilities = [0.9489778  0.03861756 0.01240465], Uncertainty (std) = [0.17391063 0.1316585  0.04235596]\n",
      "Sample 61: Predicted Label = 2, Probabilities = [0.02539706 0.03351424 0.94108874], Uncertainty (std) = [0.09309205 0.12278845 0.21582448]\n",
      "Sample 62: Predicted Label = 2, Probabilities = [0.03941463 0.05300747 0.9075779 ], Uncertainty (std) = [0.11274064 0.1515605  0.26423806]\n",
      "Sample 63: Predicted Label = 1, Probabilities = [0.03893692 0.9431433  0.01791976], Uncertainty (std) = [0.11132885 0.1625686  0.05129156]\n",
      "Sample 64: Predicted Label = 1, Probabilities = [0.05387102 0.9224354  0.02369361], Uncertainty (std) = [0.12890613 0.18562384 0.05690018]\n",
      "Sample 65: Predicted Label = 0, Probabilities = [0.9167554  0.06299311 0.02025148], Uncertainty (std) = [0.21644375 0.163806   0.05277929]\n",
      "Sample 66: Predicted Label = 2, Probabilities = [0.03655352 0.04785728 0.91558915], Uncertainty (std) = [0.11023102 0.14433776 0.25452295]\n",
      "Sample 67: Predicted Label = 2, Probabilities = [0.01805885 0.02402751 0.95791364], Uncertainty (std) = [0.07913572 0.10530438 0.18438335]\n",
      "Sample 68: Predicted Label = 2, Probabilities = [0.03560592 0.04859151 0.9158026 ], Uncertainty (std) = [0.10739428 0.14652802 0.2538858 ]\n",
      "Sample 69: Predicted Label = 1, Probabilities = [0.05334806 0.92229074 0.02436119], Uncertainty (std) = [0.12769428 0.18594766 0.05862075]\n",
      "Sample 70: Predicted Label = 0, Probabilities = [0.9488228 0.0385664 0.0126108], Uncertainty (std) = [0.17444286 0.1314807  0.0431342 ]\n",
      "Sample 71: Predicted Label = 1, Probabilities = [0.0142746  0.97945756 0.0062678 ], Uncertainty (std) = [0.07029977 0.10115001 0.03094716]\n",
      "Sample 72: Predicted Label = 2, Probabilities = [0.03239658 0.04359987 0.92400354], Uncertainty (std) = [0.10357545 0.13938828 0.24291542]\n",
      "Sample 73: Predicted Label = 1, Probabilities = [0.04658953 0.9316365  0.02177396], Uncertainty (std) = [0.12117857 0.17780122 0.05671801]\n",
      "Sample 74: Predicted Label = 2, Probabilities = [0.04316417 0.05818021 0.89865565], Uncertainty (std) = [0.11752172 0.1583804  0.2758467 ]\n",
      "Sample 75: Predicted Label = 0, Probabilities = [0.94912946 0.03853622 0.01233427], Uncertainty (std) = [0.17339054 0.13138317 0.04208769]\n",
      "Sample 76: Predicted Label = 0, Probabilities = [0.92901456 0.05336224 0.01762319], Uncertainty (std) = [0.20294571 0.152565   0.05050443]\n",
      "Sample 77: Predicted Label = 1, Probabilities = [0.03952691 0.94362336 0.01684981], Uncertainty (std) = [0.11301846 0.16118294 0.0482507 ]\n",
      "Sample 78: Predicted Label = 1, Probabilities = [0.01799988 0.9740619  0.00793819], Uncertainty (std) = [0.07886565 0.11367072 0.03487006]\n",
      "Sample 79: Predicted Label = 0, Probabilities = [0.9742363  0.01887339 0.00689029], Uncertainty (std) = [0.12690328 0.09297333 0.03429708]\n",
      "Sample 80: Predicted Label = 0, Probabilities = [0.92981684 0.05248434 0.01769885], Uncertainty (std) = [0.2006735  0.15010811 0.05073886]\n",
      "Sample 81: Predicted Label = 1, Probabilities = [0.02555111 0.9635055  0.01094335], Uncertainty (std) = [0.09362528 0.13377002 0.0401794 ]\n",
      "Sample 82: Predicted Label = 2, Probabilities = [0.03644835 0.04781566 0.9157361 ], Uncertainty (std) = [0.10991691 0.1442846  0.2541009 ]\n",
      "Sample 83: Predicted Label = 1, Probabilities = [0.04355314 0.9369946  0.01945226], Uncertainty (std) = [0.11857086 0.17154145 0.05311062]\n",
      "Sample 84: Predicted Label = 0, Probabilities = [0.95503676 0.03344322 0.01152003], Uncertainty (std) = [0.16474523 0.12253206 0.04238799]\n",
      "Sample 85: Predicted Label = 2, Probabilities = [0.03963191 0.05343551 0.9069326 ], Uncertainty (std) = [0.11332338 0.15277004 0.26607463]\n",
      "Sample 86: Predicted Label = 2, Probabilities = [0.01045638 0.01438865 0.97515506], Uncertainty (std) = [0.05976978 0.08223125 0.14199758]\n",
      "Sample 87: Predicted Label = 0, Probabilities = [0.9104469  0.06779435 0.02175884], Uncertainty (std) = [0.2230902  0.1689138  0.05437467]\n",
      "Sample 88: Predicted Label = 2, Probabilities = [0.02525089 0.03363582 0.9411133 ], Uncertainty (std) = [0.09256474 0.12330198 0.21576342]\n",
      "Sample 89: Predicted Label = 1, Probabilities = [0.02840367 0.95886785 0.01272841], Uncertainty (std) = [0.09682332 0.14021377 0.04356298]\n",
      "Sample 90: Predicted Label = 0, Probabilities = [0.9680137  0.02393649 0.00804979], Uncertainty (std) = [0.14013496 0.10488417 0.03540656]\n",
      "Sample 91: Predicted Label = 2, Probabilities = [0.03530549 0.04853939 0.91615516], Uncertainty (std) = [0.10646309 0.1463649  0.25281274]\n",
      "Sample 92: Predicted Label = 2, Probabilities = [0.04303069 0.05819254 0.89877677], Uncertainty (std) = [0.11718695 0.15840426 0.27555504]\n",
      "Sample 93: Predicted Label = 2, Probabilities = [0.04703063 0.06257223 0.89039713], Uncertainty (std) = [0.12230486 0.16274081 0.28498456]\n",
      "Sample 94: Predicted Label = 1, Probabilities = [0.04950181 0.9270937  0.02340454], Uncertainty (std) = [0.12334608 0.18167852 0.05860769]\n",
      "Sample 95: Predicted Label = 2, Probabilities = [0.02558212 0.03330537 0.9411125 ], Uncertainty (std) = [0.09372821 0.12205231 0.21574783]\n",
      "Sample 96: Predicted Label = 1, Probabilities = [0.02818062 0.95820177 0.01361761], Uncertainty (std) = [0.09608636 0.14252344 0.0466851 ]\n",
      "Sample 97: Predicted Label = 1, Probabilities = [0.01829295 0.9739444  0.00776262], Uncertainty (std) = [0.08015095 0.11416961 0.03405824]\n",
      "Sample 98: Predicted Label = 1, Probabilities = [0.02170075 0.96900785 0.00929139], Uncertainty (std) = [0.08633863 0.12329364 0.0370347 ]\n",
      "Sample 99: Predicted Label = 2, Probabilities = [0.03275847 0.04293652 0.92430496], Uncertainty (std) = [0.10475948 0.13731143 0.24193844]\n"
     ]
    }
   ],
   "source": [
    "# Example output with probabilities and uncertainty\n",
    "for i, (mean, std) in enumerate(zip(predictions_mean, predictions_std)):\n",
    "    # softmax_probs = np.exp(mean) / np.sum(np.exp(mean)) # Softmax to get probabilities\n",
    "    print(f'Sample {i}: Predicted Label = {predicted_labels[i]}, Probabilities = {mean}, Uncertainty (std) = {std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Export the reults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Save test predictions to a CSV\n",
    "test_df = pd.DataFrame({\n",
    "    'Prediction': predicted_labels,\n",
    "    'Probability_0': [p[0] for p in predictions_mean],\n",
    "    'Probability_1': [p[1] for p in predictions_mean],\n",
    "    'Probability_2': [p[2] for p in predictions_mean],  # Adjust based on num_classes\n",
    "    'Uncertainty_0': [u[0] for u in predictions_std],\n",
    "    'Uncertainty_1': [u[1] for u in predictions_std],\n",
    "    'Uncertainty_2': [u[2] for u in predictions_std] \n",
    "})\n",
    "\n",
    "test_df.to_csv('test_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "save(ae_attention_model.state_dict(), 'autoencoder_attention_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('ae_data.h5', 'w') as f:\n",
    "    f.create_dataset('X', data=data_module.val_features)\n",
    "    f.create_dataset('y', data=data_module.val_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainable AI Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('ae_data.h5', 'r') as f:\n",
    "    X = f['X'][:]\n",
    "    y = f['y'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_autoencoder_classifiers import AutoencoderAttentionClassifier\n",
    "from pl_model_utils import TimeSeriesDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TimeSeriesDataset(\n",
    "            torch.tensor(y, dtype=torch.float32), \n",
    "            torch.tensor(X, dtype=torch.float32)\n",
    "            )\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_loader))\n",
    "targets, features = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = AutoencoderAttentionClassifier(\n",
    "    context_length=1, \n",
    "    num_classes=3, \n",
    "    num_features=2,\n",
    "    lr=0.001,\n",
    "    num_heads=1,\n",
    "    dropout_prob=0.1,\n",
    "    hidden_units=256,\n",
    "    embed_dim=128,\n",
    "    classifier_units=64)\n",
    "ae_model.load_state_dict(torch.load('autoencoder_attention_classifier.pth'))\n",
    "ae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "e = shap.DeepExplainer(ae_model, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import modeling_tf_utils"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GluonTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

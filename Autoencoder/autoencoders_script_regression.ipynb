{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "cur_dir = os.getcwd()\n",
    "# Add the current directory to system path (including parent folder)\n",
    "sys.path.append(cur_dir)\n",
    "sys.path.append(os.path.split(cur_dir)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Prepare sample data\n",
    "timestamp = pd.date_range(start='2020-01-01', periods=300, freq='D')\n",
    "time_series = pd.DataFrame({'values': np.random.randn(300)}, index=timestamp)\n",
    "labels = pd.DataFrame({'label': np.random.randn(300)}, index=timestamp)\n",
    "ext_features = pd.DataFrame({\n",
    "    'feature1': np.random.randn(300),\n",
    "    'feature2': np.random.randn(300)\n",
    "}, index=timestamp)\n",
    "\n",
    "combined_data = time_series.join(labels).join(ext_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Validation / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Validation / Test Split\n",
    "train_data = combined_data[:100]\n",
    "val_data = combined_data[100:200]\n",
    "test_data = combined_data[200:]\n",
    "\n",
    "# Read targets\n",
    "train_target = train_data.label.values\n",
    "val_target = val_data.label.values\n",
    "test_target = test_data.label.values\n",
    "\n",
    "# Read features\n",
    "train_features = train_data[['feature1', 'feature2']].values\n",
    "val_features = val_data[['feature1', 'feature2']].values\n",
    "test_features = test_data[['feature1', 'feature2']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pl_model_utils import TimeSeriesDataModule\n",
    "# Instantiate data module and model\n",
    "data_module = TimeSeriesDataModule(\n",
    "    train_target, train_features,\n",
    "    val_target, val_features,\n",
    "    test_target, test_features,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Setup the data for model\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Basic Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compile simple Autoencoder for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_autoencoder_classifiers import AutoencoderClassifier\n",
    "ae_model = AutoencoderClassifier(context_length=1, num_classes=3, num_features=2, task='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='train_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00, 121.42it/s, v_num=132]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00, 103.05it/s, v_num=132]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "# Train the model\n",
    "trainer = Trainer(max_epochs=20, callbacks=[checkpoint_callback])\n",
    "trainer.fit(ae_model, data_module.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross-Validation (Using Combinatorial Purged K-Fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 112.98it/s, v_num=133]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 91.39it/s, v_num=133] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 104.91it/s, v_num=134]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 88.39it/s, v_num=134] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:06,  3.33s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 94.28it/s, v_num=135] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 80.06it/s, v_num=135]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:10,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 91.43it/s, v_num=136] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 76.04it/s, v_num=136]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:13,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 113.00it/s, v_num=137]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 94.06it/s, v_num=137] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:17,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 109.60it/s, v_num=138]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 90.87it/s, v_num=138] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:20,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 110.75it/s, v_num=139]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 90.99it/s, v_num=139] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:24,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 99.69it/s, v_num=140] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 82.65it/s, v_num=140]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:27,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 96.54it/s, v_num=141] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 79.72it/s, v_num=141]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:30,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 100.55it/s, v_num=142]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 86.20it/s, v_num=142] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:34,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:34,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Report:\n",
      "{'Fold_0': 0.78342783, 'Fold_1': 1.0746402, 'Fold_2': 0.30051118, 'Fold_3': 0.39082208, 'Fold_4': 0.21280167, 'Fold_5': 0.37165302, 'Fold_6': 0.9396434, 'Fold_7': 0.89639485, 'Fold_8': 0.68916124, 'Fold_9': 0.49383968}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pl_model_utils import cross_validate_model\n",
    "from CPCV.cpcv import CombPurgedKFoldCVLocal\n",
    "\n",
    "\n",
    "pred_times = pd.Series(val_data.index, index=val_data.index)\n",
    "eval_times = pd.Series(val_data.index, index=val_data.index)\n",
    "\n",
    "# Construct CPCV in-line with DePrado method\n",
    "cpcv = CombPurgedKFoldCVLocal(\n",
    "    n_splits=10,\n",
    "    n_test_splits=1,\n",
    "    embargo_td=pd.Timedelta(days=2)\n",
    ")\n",
    "\n",
    "cv_split = cpcv.split(\n",
    "    pd.DataFrame(data_module.val_features, index=val_data.index), \n",
    "    pd.Series(data_module.val_target, index=val_data.index), \n",
    "    pred_times, \n",
    "    eval_times)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate_model(\n",
    "    X=data_module.val_features,\n",
    "    y=data_module.val_target,\n",
    "    model=ae_model,\n",
    "    cv_split=cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folds</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fold_0</td>\n",
       "      <td>0.783428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fold_1</td>\n",
       "      <td>1.074640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fold_2</td>\n",
       "      <td>0.300511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fold_3</td>\n",
       "      <td>0.390822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fold_4</td>\n",
       "      <td>0.212802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fold_5</td>\n",
       "      <td>0.371653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fold_6</td>\n",
       "      <td>0.939643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fold_7</td>\n",
       "      <td>0.896395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fold_8</td>\n",
       "      <td>0.689161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fold_9</td>\n",
       "      <td>0.493840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Folds      Loss\n",
       "0  Fold_0  0.783428\n",
       "1  Fold_1  1.074640\n",
       "2  Fold_2  0.300511\n",
       "3  Fold_3  0.390822\n",
       "4  Fold_4  0.212802\n",
       "5  Fold_5  0.371653\n",
       "6  Fold_6  0.939643\n",
       "7  Fold_7  0.896395\n",
       "8  Fold_8  0.689161\n",
       "9  Fold_9  0.493840"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cv_results.items(), columns=['Folds', 'Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 18.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from mc_dropout import mc_dropout # final, one-line function to implement mc dropout (As suggested here: https://github.com/Lightning-AI/pytorch-lightning/blob/f35e2210e240b443fd4dafed8fe2e30ee7d579ea/docs/source/common/production_inference.rst#prediction-api)\n",
    "predictions_mean, predictions_std = mc_dropout(ae_model, data_module.val_dataloader(), mc_iterations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Attention-Enchanced Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pl_autoencoder_classifiers import AutoencoderAttentionClassifier\n",
    "from pl_model_utils import TimeSeriesDataset\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "def ae_attention_objective(trial):\n",
    "    context_length = 1\n",
    "    num_classes = 3\n",
    "    num_features = 2\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_categorical('lr', [1e-5, 1e-3, 1e-2])\n",
    "    num_heads = trial.suggest_categorical('num_heads', [1, 2, 4])\n",
    "    dropout_prob = trial.suggest_categorical('dropout_prob', [0.1, 0.3, 0.5])\n",
    "    hidden_units = trial.suggest_categorical('hidden_units', [64, 128, 256])\n",
    "    embed_dim = trial.suggest_categorical('embed_dim', [32, 64, 128])\n",
    "    # classifier_units = trial.suggest_categorical('classifier_units', [16, 32, 64])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "\n",
    "    # Initialize the model with suggested hyperparameters\n",
    "    model = AutoencoderAttentionClassifier(\n",
    "        context_length=context_length,\n",
    "        num_classes=num_classes,\n",
    "        num_features=num_features,\n",
    "        num_heads=num_heads,\n",
    "        dropout_prob=dropout_prob,\n",
    "        hidden_units=hidden_units,\n",
    "        embed_dim=embed_dim,\n",
    "        # classifier_units=classifier_units,\n",
    "        lr=lr,\n",
    "        task='regression'\n",
    "    )\n",
    "\n",
    "    # Assuming you have your dataset in `X` and `y`\n",
    "    X, y = data_module.val_features, data_module.val_target\n",
    "    # X = np.array(X_train)  # Ensure X_train is a NumPy array\n",
    "    # y = np.array(y_train)  # Ensure y_train is a NumPy array\n",
    "\n",
    "    # Time series split\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    cv_scores = []\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        # Create DataLoader for the training and validation fold\n",
    "        train_dataset = TimeSeriesDataset(\n",
    "            torch.tensor(y_train_fold, dtype=torch.float32), \n",
    "            torch.tensor(X_train_fold, dtype=torch.float32)\n",
    "            )\n",
    "        \n",
    "        val_dataset = TimeSeriesDataset(\n",
    "            torch.tensor(y_val_fold, dtype=torch.float32), \n",
    "            torch.tensor(X_val_fold, dtype=torch.float32)\n",
    "            )\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=10,\n",
    "            callbacks=[EarlyStopping(monitor='train_loss', patience=3, mode='min')],\n",
    "            logger=False,\n",
    "            enable_checkpointing=False\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.fit(model, train_loader)\n",
    "\n",
    "        # Validate the model\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                targets, features = batch\n",
    "                if model.task == 'classification': \n",
    "                    _, classification = model(targets, features)\n",
    "                    all_preds.extend(classification.cpu().numpy())\n",
    "                elif model.task == 'regression': \n",
    "                    out = model(targets, features)\n",
    "                    all_preds.extend(out.cpu().numpy())\n",
    "                # preds = torch.argmax(classification, dim=1)\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        # val_predictions = trainer.predict(model, val_loader)\n",
    "        # val_predictions = torch.cat([x for x in val_predictions], dim=0).numpy()\n",
    "        \n",
    "        val_loss = log_loss(all_targets, all_preds) if model.task == 'classification' else mean_squared_error(all_targets, all_preds)\n",
    "        cv_scores.append(val_loss)\n",
    "\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-03 14:35:06,068] A new study created in memory with name: no-name-d6aa0879-4209-44dd-9642-c821a6486dfa\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 1/2 [00:00<00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 60.38it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  33%|███▎      | 1/3 [00:00<00:00, 54.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3/3 [00:00<00:00, 46.29it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  50%|█████     | 2/4 [00:00<00:00, 68.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 4/4 [00:00<00:00, 51.02it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  40%|████      | 2/5 [00:00<00:00, 65.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 5/5 [00:00<00:00, 29.92it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  17%|█▋        | 1/6 [00:00<00:00, 39.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 6/6 [00:00<00:00, 67.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-03 14:35:08,608] Trial 0 finished with value: 0.988956093788147 and parameters: {'lr': 0.001, 'num_heads': 1, 'dropout_prob': 0.3, 'hidden_units': 128, 'embed_dim': 128, 'batch_size': 16}. Best is trial 0 with value: 0.988956093788147.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.5 K\n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 16.9 K\n",
      "-------------------------------------------------\n",
      "51.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "51.0 K    Total params\n",
      "0.204     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 78.03it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 51.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.5 K\n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 16.9 K\n",
      "-------------------------------------------------\n",
      "51.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "51.0 K    Total params\n",
      "0.204     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 61.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.5 K\n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 16.9 K\n",
      "-------------------------------------------------\n",
      "51.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "51.0 K    Total params\n",
      "0.204     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 66.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 56.68it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 52.01it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.5 K\n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 16.9 K\n",
      "-------------------------------------------------\n",
      "51.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "51.0 K    Total params\n",
      "0.204     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 1/3 [00:00<00:00, 62.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3/3 [00:00<00:00, 60.09it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.5 K\n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 16.9 K\n",
      "-------------------------------------------------\n",
      "51.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "51.0 K    Total params\n",
      "0.204     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  33%|███▎      | 1/3 [00:00<00:00, 63.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3/3 [00:00<00:00, 60.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-03 14:35:10,259] Trial 1 finished with value: 1.014594316482544 and parameters: {'lr': 1e-05, 'num_heads': 4, 'dropout_prob': 0.5, 'hidden_units': 256, 'embed_dim': 64, 'batch_size': 32}. Best is trial 0 with value: 0.988956093788147.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 33.9 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 33.3 K\n",
      "-------------------------------------------------\n",
      "133 K     Trainable params\n",
      "0         Non-trainable params\n",
      "133 K     Total params\n",
      "0.533     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 33.9 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 33.3 K\n",
      "-------------------------------------------------\n",
      "133 K     Trainable params\n",
      "0         Non-trainable params\n",
      "133 K     Total params\n",
      "0.533     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 54.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 33.9 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 33.3 K\n",
      "-------------------------------------------------\n",
      "133 K     Trainable params\n",
      "0         Non-trainable params\n",
      "133 K     Total params\n",
      "0.533     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 64.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 33.9 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 33.3 K\n",
      "-------------------------------------------------\n",
      "133 K     Trainable params\n",
      "0         Non-trainable params\n",
      "133 K     Total params\n",
      "0.533     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3/3 [00:00<00:00, 64.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3/3 [00:00<00:00, 48.87it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 33.9 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 33.3 K\n",
      "-------------------------------------------------\n",
      "133 K     Trainable params\n",
      "0         Non-trainable params\n",
      "133 K     Total params\n",
      "0.533     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3/3 [00:00<00:00, 64.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3/3 [00:00<00:00, 53.22it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-03 14:35:11,683] Trial 2 finished with value: 0.988956093788147 and parameters: {'lr': 0.01, 'num_heads': 2, 'dropout_prob': 0.1, 'hidden_units': 256, 'embed_dim': 128, 'batch_size': 32}. Best is trial 0 with value: 0.988956093788147.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 9.2 K \n",
      "1 | attention | MultiheadAttention | 4.2 K \n",
      "2 | decoder   | Sequential         | 8.7 K \n",
      "-------------------------------------------------\n",
      "22.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.2 K    Total params\n",
      "0.089     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/2 [00:00<?, ?it/s]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 41.69it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 9.2 K \n",
      "1 | attention | MultiheadAttention | 4.2 K \n",
      "2 | decoder   | Sequential         | 8.7 K \n",
      "-------------------------------------------------\n",
      "22.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.2 K    Total params\n",
      "0.089     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3/3 [00:00<00:00, 84.04it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3/3 [00:00<00:00, 68.22it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 9.2 K \n",
      "1 | attention | MultiheadAttention | 4.2 K \n",
      "2 | decoder   | Sequential         | 8.7 K \n",
      "-------------------------------------------------\n",
      "22.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.2 K    Total params\n",
      "0.089     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  75%|███████▌  | 3/4 [00:00<00:00, 72.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 4/4 [00:00<00:00, 54.08it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 9.2 K \n",
      "1 | attention | MultiheadAttention | 4.2 K \n",
      "2 | decoder   | Sequential         | 8.7 K \n",
      "-------------------------------------------------\n",
      "22.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.2 K    Total params\n",
      "0.089     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  20%|██        | 1/5 [00:00<00:00, 69.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 5/5 [00:00<00:00, 68.38it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 9.2 K \n",
      "1 | attention | MultiheadAttention | 4.2 K \n",
      "2 | decoder   | Sequential         | 8.7 K \n",
      "-------------------------------------------------\n",
      "22.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.2 K    Total params\n",
      "0.089     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  83%|████████▎ | 5/6 [00:00<00:00, 73.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 6/6 [00:00<00:00, 67.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-03 14:35:13,582] Trial 3 finished with value: 0.988956093788147 and parameters: {'lr': 0.01, 'num_heads': 2, 'dropout_prob': 0.3, 'hidden_units': 256, 'embed_dim': 32, 'batch_size': 16}. Best is trial 0 with value: 0.988956093788147.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 8.8 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 8.4 K \n",
      "-------------------------------------------------\n",
      "33.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "33.9 K    Total params\n",
      "0.135     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 59.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 43.57it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 8.8 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 8.4 K \n",
      "-------------------------------------------------\n",
      "33.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "33.9 K    Total params\n",
      "0.135     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 54.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 8.8 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 8.4 K \n",
      "-------------------------------------------------\n",
      "33.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "33.9 K    Total params\n",
      "0.135     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 57.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 8.8 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 8.4 K \n",
      "-------------------------------------------------\n",
      "33.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "33.9 K    Total params\n",
      "0.135     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  33%|███▎      | 1/3 [00:00<00:00, 71.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3/3 [00:00<00:00, 57.96it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 8.8 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 8.4 K \n",
      "-------------------------------------------------\n",
      "33.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "33.9 K    Total params\n",
      "0.135     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  33%|███▎      | 1/3 [00:00<00:00, 21.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3/3 [00:00<00:00, 60.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-03 14:35:14,966] Trial 4 finished with value: 0.988956093788147 and parameters: {'lr': 0.001, 'num_heads': 1, 'dropout_prob': 0.5, 'hidden_units': 128, 'embed_dim': 64, 'batch_size': 32}. Best is trial 0 with value: 0.988956093788147.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00, 67.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 60.20it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 60.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 57.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  33%|███▎      | 1/3 [00:00<00:00, 77.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3/3 [00:00<00:00, 61.62it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  33%|███▎      | 1/3 [00:00<00:00, 70.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3/3 [00:00<00:00, 58.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-03 14:35:16,268] Trial 5 finished with value: 0.988956093788147 and parameters: {'lr': 0.01, 'num_heads': 4, 'dropout_prob': 0.5, 'hidden_units': 64, 'embed_dim': 64, 'batch_size': 32}. Best is trial 0 with value: 0.988956093788147.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00, 61.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 38.70it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1/1 [00:00<00:00, 74.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([36])) that is different to the input size (torch.Size([36, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 43.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00, 37.45it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1/1 [00:00<00:00, 47.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([52])) that is different to the input size (torch.Size([52, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1/1 [00:00<00:00, 43.87it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|█████     | 1/2 [00:00<00:00, 37.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 57.60it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  50%|█████     | 1/2 [00:00<00:00, 76.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 58.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-03 14:35:17,938] Trial 6 finished with value: 1.0281013250350952 and parameters: {'lr': 1e-05, 'num_heads': 1, 'dropout_prob': 0.3, 'hidden_units': 64, 'embed_dim': 64, 'batch_size': 64}. Best is trial 0 with value: 0.988956093788147.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 33.9 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 33.3 K\n",
      "-------------------------------------------------\n",
      "133 K     Trainable params\n",
      "0         Non-trainable params\n",
      "133 K     Total params\n",
      "0.533     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00, 58.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00, 50.87it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 33.9 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 33.3 K\n",
      "-------------------------------------------------\n",
      "133 K     Trainable params\n",
      "0         Non-trainable params\n",
      "133 K     Total params\n",
      "0.533     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 60.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 33.9 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 33.3 K\n",
      "-------------------------------------------------\n",
      "133 K     Trainable params\n",
      "0         Non-trainable params\n",
      "133 K     Total params\n",
      "0.533     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 66.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 52.55it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 33.9 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 33.3 K\n",
      "-------------------------------------------------\n",
      "133 K     Trainable params\n",
      "0         Non-trainable params\n",
      "133 K     Total params\n",
      "0.533     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3/3 [00:00<00:00, 79.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3/3 [00:00<00:00, 43.23it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 33.9 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 33.3 K\n",
      "-------------------------------------------------\n",
      "133 K     Trainable params\n",
      "0         Non-trainable params\n",
      "133 K     Total params\n",
      "0.533     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3/3 [00:00<00:00, 67.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3/3 [00:00<00:00, 51.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-03 14:35:19,372] Trial 7 finished with value: 0.988956093788147 and parameters: {'lr': 0.001, 'num_heads': 4, 'dropout_prob': 0.1, 'hidden_units': 256, 'embed_dim': 128, 'batch_size': 32}. Best is trial 0 with value: 0.988956093788147.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  50%|█████     | 1/2 [00:00<00:00, 77.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 66.98it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  67%|██████▋   | 2/3 [00:00<00:00, 78.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 3/3 [00:00<00:00, 63.57it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 4/4 [00:00<00:00, 91.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 4/4 [00:00<00:00, 67.91it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 5/5 [00:00<00:00, 68.45it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 4.4 K \n",
      "1 | attention | MultiheadAttention | 16.6 K\n",
      "2 | decoder   | Sequential         | 4.2 K \n",
      "-------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6/6 [00:00<00:00, 78.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 6/6 [00:00<00:00, 73.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 6/6 [00:00<00:00, 70.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-03 14:35:22,268] Trial 8 finished with value: 0.9964146614074707 and parameters: {'lr': 1e-05, 'num_heads': 2, 'dropout_prob': 0.5, 'hidden_units': 64, 'embed_dim': 64, 'batch_size': 16}. Best is trial 0 with value: 0.988956093788147.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 8.6 K \n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 8.3 K \n",
      "-------------------------------------------------\n",
      "82.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "82.9 K    Total params\n",
      "0.332     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 67.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 56.82it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 8.6 K \n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 8.3 K \n",
      "-------------------------------------------------\n",
      "82.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "82.9 K    Total params\n",
      "0.332     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  33%|███▎      | 1/3 [00:00<00:00, 83.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3/3 [00:00<00:00, 57.45it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 8.6 K \n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 8.3 K \n",
      "-------------------------------------------------\n",
      "82.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "82.9 K    Total params\n",
      "0.332     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 4/4 [00:00<00:00, 73.02it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 4/4 [00:00<00:00, 60.90it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 8.6 K \n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 8.3 K \n",
      "-------------------------------------------------\n",
      "82.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "82.9 K    Total params\n",
      "0.332     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 5/5 [00:00<00:00, 66.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 5/5 [00:00<00:00, 63.12it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 8.6 K \n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 8.3 K \n",
      "-------------------------------------------------\n",
      "82.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "82.9 K    Total params\n",
      "0.332     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  67%|██████▋   | 4/6 [00:00<00:00, 78.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 6/6 [00:00<00:00, 69.63it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-03 14:35:24,126] Trial 9 finished with value: 0.988956093788147 and parameters: {'lr': 0.01, 'num_heads': 4, 'dropout_prob': 0.5, 'hidden_units': 64, 'embed_dim': 128, 'batch_size': 16}. Best is trial 0 with value: 0.988956093788147.\n"
     ]
    }
   ],
   "source": [
    "# Run the Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(ae_attention_objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.001, 'num_heads': 1, 'dropout_prob': 0.3, 'hidden_units': 128, 'embed_dim': 128, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_attention_model = AutoencoderAttentionClassifier(\n",
    "    context_length=1, \n",
    "    num_classes=3, # remove this reduntant param\n",
    "    num_features=2,\n",
    "    lr=study.best_params['lr'],\n",
    "    num_heads=study.best_params['num_heads'],\n",
    "    dropout_prob=study.best_params['dropout_prob'],\n",
    "    hidden_units=study.best_params['hidden_units'],\n",
    "    embed_dim=study.best_params['embed_dim'],\n",
    "    # classifier_units=study.best_params['classifier_units'],\n",
    "    task='regression'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_attention_model = AutoencoderAttentionClassifier(\n",
    "    context_length=1, \n",
    "    num_classes=3, \n",
    "    num_features=2,\n",
    "    lr=0.001,\n",
    "    num_heads=1,\n",
    "    dropout_prob=0.1,\n",
    "    hidden_units=256,\n",
    "    embed_dim=128,\n",
    "    classifier_units=64,\n",
    "    task='regression'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:612: UserWarning: Checkpoint directory c:\\Users\\jedre\\OneDrive\\Pulpit\\State_Street\\Programs\\Python\\ML_Research_Project_2024\\Autoencoder\\lightning_logs\\version_132\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/7 [00:00<?, ?it/s, v_num=154]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 7/7 [00:00<00:00, 67.06it/s, v_num=154] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 7/7 [00:00<00:00, 58.86it/s, v_num=154]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer = Trainer(max_epochs=100, callbacks=[checkpoint_callback])\n",
    "trainer.fit(ae_attention_model, data_module.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/6 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  17%|█▋        | 1/6 [00:00<00:00, 74.77it/s, v_num=165]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 55.43it/s, v_num=165]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 44.35it/s, v_num=165]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [07:31, 451.58s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 3/6 [00:00<00:00, 43.22it/s, v_num=166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 6/6 [00:00<00:00, 45.34it/s, v_num=166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 53.28it/s, v_num=166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 42.18it/s, v_num=166]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [07:39, 190.66s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  17%|█▋        | 1/6 [00:00<00:00, 51.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/6 [00:00<?, ?it/s, v_num=167]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 63.72it/s, v_num=167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 47.84it/s, v_num=167]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [07:46, 106.83s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  83%|████████▎ | 5/6 [00:00<00:00, 51.21it/s, v_num=168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/6 [00:00<?, ?it/s, v_num=168]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 55.05it/s, v_num=168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 43.34it/s, v_num=168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [07:53, 67.45s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  17%|█▋        | 1/6 [00:00<00:00, 49.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/6 [00:00<?, ?it/s, v_num=169]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 72.53it/s, v_num=169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 57.88it/s, v_num=169]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [07:59, 45.22s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  33%|███▎      | 2/6 [00:00<00:00, 75.34it/s, v_num=170]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/6 [00:00<?, ?it/s, v_num=170]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 44.06it/s, v_num=170]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 35.63it/s, v_num=170]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [08:06, 32.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 6/6 [00:00<00:00, 66.29it/s, v_num=171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 53.63it/s, v_num=171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 43.00it/s, v_num=171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [08:13, 23.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/6 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 6/6 [00:00<00:00, 52.14it/s, v_num=172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 38.99it/s, v_num=172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 31.42it/s, v_num=172]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [08:22, 19.30s/it]GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  67%|██████▋   | 4/6 [00:00<00:00, 50.28it/s, v_num=173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 6/6 [00:00<00:00, 46.63it/s, v_num=173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 33.20it/s, v_num=173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 29.12it/s, v_num=173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [08:31, 16.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential         | 17.0 K\n",
      "1 | attention | MultiheadAttention | 66.0 K\n",
      "2 | decoder   | Sequential         | 16.6 K\n",
      "-------------------------------------------------\n",
      "99.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "99.7 K    Total params\n",
      "0.399     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  33%|███▎      | 2/6 [00:00<00:00, 51.84it/s, v_num=174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/6 [00:00<?, ?it/s, v_num=174]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 66.03it/s, v_num=174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 53.67it/s, v_num=174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [08:37, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [08:37, 51.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Report:\n",
      "{'Fold_0': 0.9438299, 'Fold_1': 1.7545269, 'Fold_2': 0.8643959, 'Fold_3': 0.40900922, 'Fold_4': 0.4904519, 'Fold_5': 1.0080007, 'Fold_6': 1.1333838, 'Fold_7': 1.3080504, 'Fold_8': 0.94994783, 'Fold_9': 0.84711444}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct CPCV in-line with DePrado method\n",
    "cpcv = CombPurgedKFoldCVLocal(\n",
    "    n_splits=10,\n",
    "    n_test_splits=1,\n",
    "    embargo_td=pd.Timedelta(days=2)\n",
    ")\n",
    "\n",
    "cv_split = cpcv.split(\n",
    "    pd.DataFrame(data_module.val_features, index=val_data.index), \n",
    "    pd.Series(data_module.val_target, index=val_data.index), \n",
    "    pred_times, \n",
    "    eval_times)\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate_model(\n",
    "    X=data_module.val_features, \n",
    "    y=data_module.val_target, \n",
    "    model=ae_attention_model,\n",
    "    cv_split=cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folds</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fold_0</td>\n",
       "      <td>0.943830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fold_1</td>\n",
       "      <td>1.754527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fold_2</td>\n",
       "      <td>0.864396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fold_3</td>\n",
       "      <td>0.409009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fold_4</td>\n",
       "      <td>0.490452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fold_5</td>\n",
       "      <td>1.008001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fold_6</td>\n",
       "      <td>1.133384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fold_7</td>\n",
       "      <td>1.308050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fold_8</td>\n",
       "      <td>0.949948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fold_9</td>\n",
       "      <td>0.847114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Folds      Loss\n",
       "0  Fold_0  0.943830\n",
       "1  Fold_1  1.754527\n",
       "2  Fold_2  0.864396\n",
       "3  Fold_3  0.409009\n",
       "4  Fold_4  0.490452\n",
       "5  Fold_5  1.008001\n",
       "6  Fold_6  1.133384\n",
       "7  Fold_7  1.308050\n",
       "8  Fold_8  0.949948\n",
       "9  Fold_9  0.847114"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cv_results.items(), columns=['Folds', 'Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  5.45it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_mean, predictions_std = mc_dropout(ae_attention_model, data_module.val_dataloader(), mc_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Prediction = 0.0, Uncertainty (std) = 0.0\n",
      "Sample 1: Prediction = 0.0, Uncertainty (std) = 0.0\n",
      "Sample 2: Prediction = 0.0, Uncertainty (std) = 0.0\n",
      "Sample 3: Prediction = 0.0, Uncertainty (std) = 0.0\n",
      "Sample 4: Prediction = 0.0, Uncertainty (std) = 0.0\n",
      "Sample 5: Prediction = 0.0, Uncertainty (std) = 0.0\n",
      "Sample 6: Prediction = 0.0, Uncertainty (std) = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Example output with probabilities and uncertainty\n",
    "for i, (mean, std) in enumerate(zip(predictions_mean, predictions_std)):\n",
    "    # softmax_probs = np.exp(mean) / np.sum(np.exp(mean)) # Softmax to get probabilities\n",
    "    print(f'Sample {i}: Prediction = {mean}, Uncertainty (std) = {std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Export the reults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Save test predictions to a CSV\n",
    "test_df = pd.DataFrame({\n",
    "    'Prediction': predictions_mean,\n",
    "    'Uncertainty': predictions_std\n",
    "})\n",
    "\n",
    "test_df.to_csv('test_predictions_reg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "save(ae_attention_model.state_dict(), 'autoencoder_attention_regressor.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('ae_data.h5', 'w') as f:\n",
    "    f.create_dataset('X', data=data_module.val_features)\n",
    "    f.create_dataset('y', data=data_module.val_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainable AI Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('ae_data.h5', 'r') as f:\n",
    "    X = f['X'][:]\n",
    "    y = f['y'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_autoencoder_classifiers import AutoencoderAttentionClassifier\n",
    "from pl_model_utils import TimeSeriesDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "val_dataset = TimeSeriesDataset(\n",
    "            torch.tensor(y, dtype=torch.float32), \n",
    "            torch.tensor(X, dtype=torch.float32)\n",
    "            )\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_loader))\n",
    "targets, features = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat((\n",
    "    targets.unsqueeze(1),\n",
    "    features\n",
    "), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AutoencoderAttentionClassifier:\n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([128, 3]) from checkpoint, the shape in current model is torch.Size([256, 3]).\n\tsize mismatch for encoder.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.3.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for decoder.0.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for decoder.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.2.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m ae_model \u001b[38;5;241m=\u001b[39m AutoencoderAttentionClassifier(\n\u001b[0;32m      2\u001b[0m     context_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[0;32m      3\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# classifier_units=64,\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mae_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mautoencoder_attention_regressor.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m ae_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AutoencoderAttentionClassifier:\n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([128, 3]) from checkpoint, the shape in current model is torch.Size([256, 3]).\n\tsize mismatch for encoder.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder.3.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for decoder.0.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for decoder.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for decoder.2.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 256])."
     ]
    }
   ],
   "source": [
    "ae_model = AutoencoderAttentionClassifier(\n",
    "    context_length=1, \n",
    "    num_classes=3, \n",
    "    num_features=2,\n",
    "    lr=0.001,\n",
    "    num_heads=1,\n",
    "    dropout_prob=0.1,\n",
    "    hidden_units=256,\n",
    "    embed_dim=128,\n",
    "    # classifier_units=64,\n",
    "    task='regression')\n",
    "ae_model.load_state_dict(torch.load('autoencoder_attention_regressor.pth'))\n",
    "ae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "e = shap.DeepExplainer(ae_model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = e.shap_values(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x270 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap.summary_plot(shap_values[:,:,0], x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GluonTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "cur_dir = os.getcwd()\n",
    "# Add the current directory to system path\n",
    "sys.path.append(cur_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Prepare sample data\n",
    "timestamp = pd.date_range(start='2020-01-01', periods=300, freq='D')\n",
    "time_series = pd.DataFrame({'values': np.random.randn(300)}, index=timestamp)\n",
    "labels = pd.DataFrame({'label': np.random.randint(0, 3, size=300)}, index=timestamp)\n",
    "ext_features = pd.DataFrame({\n",
    "    'feature1': np.random.randn(300),\n",
    "    'feature2': np.random.randn(300)\n",
    "}, index=timestamp)\n",
    "\n",
    "combined_data = time_series.join(labels).join(ext_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Validation / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Validation / Test Split\n",
    "train_data = combined_data[:100]\n",
    "val_data = combined_data[100:200]\n",
    "test_data = combined_data[200:]\n",
    "\n",
    "# Read targets\n",
    "train_target = train_data.label.values\n",
    "val_target = val_data.label.values\n",
    "test_target = test_data.label.values\n",
    "\n",
    "# Read features\n",
    "train_features = train_data[['feature1', 'feature2']].values\n",
    "val_features = val_data[['feature1', 'feature2']].values\n",
    "test_features = test_data[['feature1', 'feature2']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pl_model_utils import TimeSeriesDataModule\n",
    "# Instantiate data module and model\n",
    "data_module = TimeSeriesDataModule(\n",
    "    train_target, train_features,\n",
    "    val_target, val_features,\n",
    "    test_target, test_features,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Setup the data for model\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Basic Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compile simple Autoencoder for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_autoencoder_classifiers import AutoencoderClassifier\n",
    "ae_model = AutoencoderClassifier(context_length=1, num_classes=3, num_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# Define a checkpoint callback to save the best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='train_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00, 106.64it/s, v_num=52]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00, 84.29it/s, v_num=52] \n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "# Train the model\n",
    "trainer = Trainer(max_epochs=20, callbacks=[checkpoint_callback])\n",
    "trainer.fit(ae_model, data_module.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 2/2 [00:00<00:00, 66.78it/s, v_num=53] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 2/2 [00:00<00:00, 46.94it/s, v_num=53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 3/3 [00:00<00:00, 72.41it/s, v_num=54] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 3/3 [00:00<00:00, 52.23it/s, v_num=54]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 4/4 [00:00<00:00, 61.84it/s, v_num=55] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 4/4 [00:00<00:00, 48.41it/s, v_num=55]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 5/5 [00:00<00:00, 58.53it/s, v_num=56] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 5/5 [00:00<00:00, 49.58it/s, v_num=56]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | encoder    | Sequential | 8.8 K \n",
      "1 | decoder    | Sequential | 8.4 K \n",
      "2 | classifier | Sequential | 2.2 K \n",
      "3 | softmax    | Softmax    | 0     \n",
      "------------------------------------------\n",
      "19.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.4 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 95.32it/s, v_num=57] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 78.14it/s, v_num=57]\n",
      "Cross-Validation Classification Report:\n",
      "           0.0       1.0       2.0  accuracy  macro avg  weighted avg\n",
      "precision  1.0  0.960000  1.000000    0.9875   0.986667      0.990000\n",
      "recall     1.0  1.000000  0.966667    0.9875   0.988889      0.987500\n",
      "f1-score   1.0  0.977778  0.981818    0.9875   0.986532      0.987626\n",
      "support    6.0  5.400000  4.600000    0.9875  16.000000     16.000000\n"
     ]
    }
   ],
   "source": [
    "from pl_model_utils import cross_validate_model\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate_model(data_module.val_features, data_module.val_target, AutoencoderClassifier, context_length=1, num_classes=3, num_features=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predictions with MC Dropout Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_model_utils import mc_dropout_predictions\n",
    "# Perform MC Dropout predictions\n",
    "mc_predictions = mc_dropout_predictions(ae_model, data_module.test_dataloader())\n",
    "# Calculate mean and standard deviation for uncertainty estimates\n",
    "mean_predictions = mc_predictions.mean(axis=0)\n",
    "std_predictions = mc_predictions.std(axis=0)\n",
    "# Convert mean predictions to class labels\n",
    "predicted_labels = np.argmax(mean_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Attention-Enchanced Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pl_autoencoder_classifiers import AutoencoderAttentionClassifier\n",
    "from pl_model_utils import TimeSeriesDataset\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from cpcv import CombPurgedKFoldCVLocal\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import log_loss\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "def ae_attention_objective(trial):\n",
    "    context_length = 1\n",
    "    num_classes = 3\n",
    "    num_features = 2\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_categorical('lr', [1e-5, 1e-3, 1e-2])\n",
    "    num_heads = trial.suggest_categorical('num_heads', [1, 2, 4])\n",
    "    dropout_prob = trial.suggest_categorical('dropout_prob', [0.1, 0.3, 0.5])\n",
    "    hidden_units = trial.suggest_categorical('hidden_units', [64, 128, 256])\n",
    "    embed_dim = trial.suggest_categorical('embed_dim', [32, 64, 128])\n",
    "    classifier_units = trial.suggest_categorical('classifier_units', [16, 32, 64])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "\n",
    "    # Initialize the model with suggested hyperparameters\n",
    "    model = AutoencoderAttentionClassifier(\n",
    "        context_length=context_length,\n",
    "        num_classes=num_classes,\n",
    "        num_features=num_features,\n",
    "        num_heads=num_heads,\n",
    "        dropout_prob=dropout_prob,\n",
    "        hidden_units=hidden_units,\n",
    "        embed_dim=embed_dim,\n",
    "        classifier_units=classifier_units,\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    # Assuming you have your dataset in `X` and `y`\n",
    "    X, y = data_module.val_features, data_module.val_target\n",
    "    # X = np.array(X_train)  # Ensure X_train is a NumPy array\n",
    "    # y = np.array(y_train)  # Ensure y_train is a NumPy array\n",
    "\n",
    "    pred_times = pd.Series(df.index, index=df.index)\n",
    "    eval_times = pd.Series(df.index, index=df.index)\n",
    "    \n",
    "    # Time series split\n",
    "    cpcv = CombPurgedKFoldCVLocal(\n",
    "        n_splits=10,\n",
    "        n_test_splits=1,\n",
    "        embargo_td=pd.Timedelta(days=2)\n",
    "        )\n",
    "        \n",
    "    cv_scores = []\n",
    "\n",
    "    for train_index, val_index in cpcv.split(X, y, pred_times, eval_times):\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        # Create DataLoader for the training and validation fold\n",
    "        train_dataset = TimeSeriesDataset(\n",
    "            torch.tensor(y_train_fold, dtype=torch.float32), \n",
    "            torch.tensor(X_train_fold, dtype=torch.float32)\n",
    "            )\n",
    "        \n",
    "        val_dataset = TimeSeriesDataset(\n",
    "            torch.tensor(y_val_fold, dtype=torch.float32), \n",
    "            torch.tensor(X_val_fold, dtype=torch.float32)\n",
    "            )\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=10,\n",
    "            callbacks=[EarlyStopping(monitor='train_loss', patience=3, mode='min')],\n",
    "            logger=False,\n",
    "            enable_checkpointing=False\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.fit(model, train_loader)\n",
    "\n",
    "        # Validate the model\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                targets, features = batch\n",
    "                _, classification = model(targets, features)\n",
    "                # preds = torch.argmax(classification, dim=1)\n",
    "                all_preds.extend(classification.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        # val_predictions = trainer.predict(model, val_loader)\n",
    "        # val_predictions = torch.cat([x for x in val_predictions], dim=0).numpy()\n",
    "        \n",
    "        val_loss = log_loss(all_targets, all_preds)\n",
    "        cv_scores.append(val_loss)\n",
    "\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(ae_attention_objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_attention_model = AutoencoderAttentionClassifier(\n",
    "    context_length=1, \n",
    "    num_classes=3, \n",
    "    num_features=2,\n",
    "    lr=study.best_params['lr'],\n",
    "    num_heads=study.best_params['num_heads'],\n",
    "    dropout_prob=study.best_params['dropout_prob'],\n",
    "    hidden_units=study.best_params['hidden_units'],\n",
    "    embed_dim=study.best_params['embed_dim'],\n",
    "    classifier_units=study.best_params['classifier_units']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_attention_model = AutoencoderAttentionClassifier(\n",
    "    context_length=1, \n",
    "    num_classes=3, \n",
    "    num_features=2,\n",
    "    lr=0.001,\n",
    "    num_heads=1,\n",
    "    dropout_prob=0.1,\n",
    "    hidden_units=256,\n",
    "    embed_dim=128,\n",
    "    classifier_units=64\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:612: UserWarning: Checkpoint directory c:\\Users\\jedre\\OneDrive\\Pulpit\\State_Street\\Programs\\Python\\ML_Research_Project_2024\\Autoencoder\\lightning_logs\\version_52\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00, 37.59it/s, v_num=58]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00, 34.12it/s, v_num=58]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer = Trainer(max_epochs=20, callbacks=[checkpoint_callback])\n",
    "trainer.fit(ae_attention_model, data_module.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 2/2 [00:00<00:00, 53.80it/s, v_num=59]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 2/2 [00:00<00:00, 36.77it/s, v_num=59]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 3/3 [00:00<00:00, 52.36it/s, v_num=60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 3/3 [00:00<00:00, 40.40it/s, v_num=60]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 4/4 [00:00<00:00, 66.01it/s, v_num=61] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 4/4 [00:00<00:00, 52.06it/s, v_num=61]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 5/5 [00:00<00:00, 36.98it/s, v_num=62]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 5/5 [00:00<00:00, 32.12it/s, v_num=62]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder    | Sequential         | 33.9 K\n",
      "1 | attention  | MultiheadAttention | 66.0 K\n",
      "2 | decoder    | Sequential         | 33.3 K\n",
      "3 | classifier | Sequential         | 8.5 K \n",
      "4 | softmax    | Softmax            | 0     \n",
      "--------------------------------------------------\n",
      "141 K     Trainable params\n",
      "0         Non-trainable params\n",
      "141 K     Total params\n",
      "0.567     Total estimated model params size (MB)\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\jedre\\miniconda3\\envs\\GluonTS\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 67.63it/s, v_num=63]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:00<00:00, 56.48it/s, v_num=63]\n",
      "Cross-Validation Classification Report:\n",
      "           0.0  1.0  2.0  accuracy  macro avg  weighted avg\n",
      "precision  1.0  1.0  1.0       1.0        1.0           1.0\n",
      "recall     1.0  1.0  1.0       1.0        1.0           1.0\n",
      "f1-score   1.0  1.0  1.0       1.0        1.0           1.0\n",
      "support    6.0  5.4  4.6       1.0       16.0          16.0\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "cv_results = cross_validate_model(\n",
    "    data_module.val_features, \n",
    "    data_module.val_target, \n",
    "    AutoencoderAttentionClassifier, \n",
    "    context_length=1, \n",
    "    num_classes=3, \n",
    "    num_features=2,\n",
    "    lr=study.best_params['lr'],\n",
    "    num_heads=study.best_params['num_heads'],\n",
    "    dropout_prob=study.best_params['dropout_prob'],\n",
    "    hidden_units=study.best_params['hidden_units'],\n",
    "    embed_dim=study.best_params['embed_dim'],\n",
    "    classifier_units=study.best_params['classifier_units'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0.0  1.0  2.0  accuracy  macro avg  weighted avg\n",
      "precision  1.0  1.0  1.0       1.0        1.0           1.0\n",
      "recall     1.0  1.0  1.0       1.0        1.0           1.0\n",
      "f1-score   1.0  1.0  1.0       1.0        1.0           1.0\n",
      "support    6.0  5.4  4.6       1.0       16.0          16.0\n"
     ]
    }
   ],
   "source": [
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform MC Dropout predictions\n",
    "mc_predictions = mc_dropout_predictions(ae_attention_model, data_module.test_dataloader())\n",
    "# Calculate mean and standard deviation for uncertainty estimates\n",
    "mean_predictions = mc_predictions.mean(axis=0)\n",
    "std_predictions = mc_predictions.std(axis=0)\n",
    "# Convert mean predictions to class labels\n",
    "predicted_labels = np.argmax(mean_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Predicted Label = 0, Probabilities = [9.9999952e-01 5.3368467e-07 9.5207750e-14], Uncertainty (std) = [2.7844749e-06 2.7842830e-06 7.2244722e-13]\n",
      "Sample 1: Predicted Label = 2, Probabilities = [3.140716e-13 2.622725e-06 9.999975e-01], Uncertainty (std) = [2.5197552e-12 1.9383302e-05 1.9386363e-05]\n",
      "Sample 2: Predicted Label = 2, Probabilities = [3.4909291e-12 3.0534925e-06 9.9999708e-01], Uncertainty (std) = [1.5888625e-11 1.1350981e-05 1.1354489e-05]\n",
      "Sample 3: Predicted Label = 1, Probabilities = [5.5348803e-05 9.9750680e-01 2.4377755e-03], Uncertainty (std) = [0.00031489 0.01133862 0.01134168]\n",
      "Sample 4: Predicted Label = 1, Probabilities = [7.0818415e-04 9.9855006e-01 7.4174500e-04], Uncertainty (std) = [0.0025205  0.00299391 0.00154708]\n",
      "Sample 5: Predicted Label = 1, Probabilities = [8.8841058e-05 9.9986959e-01 4.1567204e-05], Uncertainty (std) = [0.00022357 0.00025558 0.00013787]\n",
      "Sample 6: Predicted Label = 1, Probabilities = [4.1749119e-04 9.9933887e-01 2.4378397e-04], Uncertainty (std) = [0.00172169 0.00175547 0.0005154 ]\n",
      "Sample 7: Predicted Label = 0, Probabilities = [9.9999410e-01 6.0767493e-06 2.0537291e-14], Uncertainty (std) = [2.7369682e-05 2.7371780e-05 8.9861488e-14]\n",
      "Sample 8: Predicted Label = 0, Probabilities = [9.9999970e-01 3.9133297e-07 4.3514300e-13], Uncertainty (std) = [1.2707397e-06 1.2620818e-06 3.2603798e-12]\n",
      "Sample 9: Predicted Label = 2, Probabilities = [3.7271895e-13 1.3992387e-06 9.9999881e-01], Uncertainty (std) = [1.5324035e-12 4.9535597e-06 4.9608684e-06]\n",
      "Sample 10: Predicted Label = 2, Probabilities = [6.2196671e-12 1.6660328e-05 9.9998355e-01], Uncertainty (std) = [3.3773220e-11 9.6318247e-05 9.6322176e-05]\n",
      "Sample 11: Predicted Label = 1, Probabilities = [8.3883700e-05 9.9563962e-01 4.2764354e-03], Uncertainty (std) = [0.00023651 0.03569924 0.03570656]\n",
      "Sample 12: Predicted Label = 0, Probabilities = [9.9999833e-01 1.7649578e-06 1.6283050e-12], Uncertainty (std) = [9.908178e-06 9.902629e-06 1.459524e-11]\n",
      "Sample 13: Predicted Label = 1, Probabilities = [2.1499641e-04 9.9932611e-01 4.5867331e-04], Uncertainty (std) = [0.00035837 0.00105889 0.000952  ]\n",
      "Sample 14: Predicted Label = 0, Probabilities = [9.9999160e-01 8.3266905e-06 7.9372938e-16], Uncertainty (std) = [7.4288575e-05 7.4284406e-05 3.5391900e-15]\n",
      "Sample 15: Predicted Label = 0, Probabilities = [9.9999964e-01 4.9097213e-07 8.5312784e-13], Uncertainty (std) = [1.4970618e-06 1.4888559e-06 7.1742985e-12]\n",
      "Sample 16: Predicted Label = 2, Probabilities = [9.3103476e-12 2.5995912e-06 9.9999750e-01], Uncertainty (std) = [7.90703961e-11 1.13941624e-05 1.13984715e-05]\n",
      "Sample 17: Predicted Label = 2, Probabilities = [1.8439562e-11 7.2794384e-04 9.9927211e-01], Uncertainty (std) = [1.04739335e-10 4.34510550e-03 4.34510317e-03]\n",
      "Sample 18: Predicted Label = 1, Probabilities = [1.8843374e-04 9.9856019e-01 1.2513028e-03], Uncertainty (std) = [0.00057434 0.00598224 0.00591638]\n",
      "Sample 19: Predicted Label = 2, Probabilities = [1.6233156e-12 7.1412046e-06 9.9999291e-01], Uncertainty (std) = [8.6607709e-12 2.8163733e-05 2.8163349e-05]\n",
      "Sample 20: Predicted Label = 1, Probabilities = [1.8510209e-04 9.9824768e-01 1.5671136e-03], Uncertainty (std) = [0.00099383 0.01113025 0.01110958]\n",
      "Sample 21: Predicted Label = 0, Probabilities = [9.9999827e-01 1.8948717e-06 1.4773890e-12], Uncertainty (std) = [6.2170016e-06 6.2182244e-06 8.6487692e-12]\n",
      "Sample 22: Predicted Label = 0, Probabilities = [9.9998415e-01 1.5850144e-05 6.6917977e-13], Uncertainty (std) = [8.7269575e-05 8.7271867e-05 4.2372165e-12]\n",
      "Sample 23: Predicted Label = 0, Probabilities = [9.9999917e-01 1.0298430e-06 4.9112671e-12], Uncertainty (std) = [2.8842737e-06 2.8769498e-06 3.1080988e-11]\n",
      "Sample 24: Predicted Label = 2, Probabilities = [1.7208953e-11 2.1763006e-04 9.9978250e-01], Uncertainty (std) = [1.0162353e-10 9.0917951e-04 9.0917817e-04]\n",
      "Sample 25: Predicted Label = 0, Probabilities = [9.9999863e-01 1.6024745e-06 4.7230142e-10], Uncertainty (std) = [7.634058e-06 7.633635e-06 4.690716e-09]\n",
      "Sample 26: Predicted Label = 0, Probabilities = [9.999878e-01 1.237640e-05 1.295340e-11], Uncertainty (std) = [6.6598543e-05 6.6595552e-05 1.1304745e-10]\n",
      "Sample 27: Predicted Label = 0, Probabilities = [9.9999982e-01 2.4315005e-07 3.6470376e-13], Uncertainty (std) = [7.4874441e-07 7.4437253e-07 1.6774453e-12]\n",
      "Sample 28: Predicted Label = 0, Probabilities = [9.9999338e-01 6.8984523e-06 4.2813314e-13], Uncertainty (std) = [2.6497606e-05 2.6496586e-05 2.4685492e-12]\n",
      "Sample 29: Predicted Label = 2, Probabilities = [2.2056723e-11 3.2641197e-05 9.9996775e-01], Uncertainty (std) = [1.9402210e-10 2.5842965e-04 2.5842505e-04]\n",
      "Sample 30: Predicted Label = 0, Probabilities = [9.9999946e-01 6.6861730e-07 9.1783785e-13], Uncertainty (std) = [3.1616992e-06 3.1547638e-06 6.8925772e-12]\n",
      "Sample 31: Predicted Label = 1, Probabilities = [1.2443744e-04 9.9864024e-01 1.2352928e-03], Uncertainty (std) = [0.00033051 0.00580418 0.00581339]\n",
      "Sample 32: Predicted Label = 0, Probabilities = [9.9999964e-01 3.8120083e-07 1.4258892e-14], Uncertainty (std) = [2.2809506e-06 2.2860850e-06 5.6406882e-14]\n",
      "Sample 33: Predicted Label = 1, Probabilities = [5.5871205e-04 9.9881572e-01 6.2553620e-04], Uncertainty (std) = [0.00143273 0.0022233  0.00167305]\n",
      "Sample 34: Predicted Label = 1, Probabilities = [1.9589753e-04 9.9915451e-01 6.4945203e-04], Uncertainty (std) = [0.00042669 0.00234628 0.00219248]\n",
      "Sample 35: Predicted Label = 2, Probabilities = [1.3164164e-10 8.3659302e-07 9.9999923e-01], Uncertainty (std) = [1.2515234e-09 2.8155707e-06 2.8138695e-06]\n",
      "Sample 36: Predicted Label = 1, Probabilities = [1.4202006e-04 9.9899429e-01 8.6369342e-04], Uncertainty (std) = [0.00034985 0.00268446 0.00261869]\n",
      "Sample 37: Predicted Label = 2, Probabilities = [3.5345275e-13 1.2273612e-06 9.9999887e-01], Uncertainty (std) = [1.2403934e-12 3.5698611e-06 3.5713281e-06]\n",
      "Sample 38: Predicted Label = 0, Probabilities = [9.9999976e-01 3.9897648e-07 1.2629959e-13], Uncertainty (std) = [1.2306880e-06 1.2208612e-06 8.3983525e-13]\n",
      "Sample 39: Predicted Label = 1, Probabilities = [1.2195790e-04 9.9982935e-01 4.8752609e-05], Uncertainty (std) = [0.00037784 0.00044679 0.0002314 ]\n",
      "Sample 40: Predicted Label = 0, Probabilities = [9.9999923e-01 8.5315185e-07 7.2630893e-13], Uncertainty (std) = [4.6177406e-06 4.6183386e-06 5.6990072e-12]\n",
      "Sample 41: Predicted Label = 2, Probabilities = [4.4443694e-13 5.6145723e-07 9.9999964e-01], Uncertainty (std) = [2.1380202e-12 1.0772563e-06 1.1033784e-06]\n",
      "Sample 42: Predicted Label = 1, Probabilities = [2.5361692e-04 9.9937552e-01 3.7093621e-04], Uncertainty (std) = [0.000501   0.00160168 0.00151192]\n",
      "Sample 43: Predicted Label = 1, Probabilities = [5.7489751e-04 9.9938929e-01 3.5912693e-05], Uncertainty (std) = [0.00264247 0.00264014 0.00013874]\n",
      "Sample 44: Predicted Label = 2, Probabilities = [5.0831722e-12 2.0592502e-06 9.9999809e-01], Uncertainty (std) = [4.4112002e-11 6.6069283e-06 6.6091202e-06]\n",
      "Sample 45: Predicted Label = 2, Probabilities = [3.9095975e-13 2.4424342e-06 9.9999779e-01], Uncertainty (std) = [1.9242453e-12 8.8459110e-06 8.8460283e-06]\n",
      "Sample 46: Predicted Label = 1, Probabilities = [3.6160526e-04 9.9939835e-01 2.3986066e-04], Uncertainty (std) = [0.00095246 0.00108999 0.00046311]\n",
      "Sample 47: Predicted Label = 2, Probabilities = [2.8411995e-11 1.9218841e-03 9.9807829e-01], Uncertainty (std) = [1.4241390e-10 1.5180940e-02 1.5180939e-02]\n",
      "Sample 48: Predicted Label = 1, Probabilities = [1.0940535e-04 9.9971467e-01 1.7581262e-04], Uncertainty (std) = [0.00043034 0.00115351 0.00107747]\n",
      "Sample 49: Predicted Label = 2, Probabilities = [7.1221058e-14 1.5032095e-06 9.9999863e-01], Uncertainty (std) = [5.3412011e-13 5.5392802e-06 5.5429246e-06]\n",
      "Sample 50: Predicted Label = 2, Probabilities = [4.6773568e-11 5.6004097e-05 9.9994415e-01], Uncertainty (std) = [3.7387399e-10 3.9693346e-04 3.9693093e-04]\n",
      "Sample 51: Predicted Label = 0, Probabilities = [9.9999970e-01 3.5667821e-07 1.3888444e-12], Uncertainty (std) = [1.0185944e-06 1.0110425e-06 1.0190314e-11]\n",
      "Sample 52: Predicted Label = 1, Probabilities = [3.0303607e-04 9.9876982e-01 9.2705578e-04], Uncertainty (std) = [0.00079711 0.00319853 0.00310383]\n",
      "Sample 53: Predicted Label = 1, Probabilities = [3.6379788e-04 9.9927348e-01 3.6271472e-04], Uncertainty (std) = [0.00072283 0.00126432 0.00106267]\n",
      "Sample 54: Predicted Label = 2, Probabilities = [9.3299205e-15 9.4901981e-05 9.9990517e-01], Uncertainty (std) = [5.1792836e-14 8.5644767e-04 8.5645192e-04]\n",
      "Sample 55: Predicted Label = 2, Probabilities = [2.9674974e-11 7.1716058e-05 9.9992853e-01], Uncertainty (std) = [1.7233261e-10 3.4443373e-04 3.4443688e-04]\n",
      "Sample 56: Predicted Label = 1, Probabilities = [7.4998557e-04 9.9898225e-01 2.6781214e-04], Uncertainty (std) = [0.00143884 0.0015835  0.00048533]\n",
      "Sample 57: Predicted Label = 1, Probabilities = [3.0299384e-04 9.9930590e-01 3.9127719e-04], Uncertainty (std) = [0.0005885  0.00113037 0.00090563]\n",
      "Sample 58: Predicted Label = 2, Probabilities = [5.6026787e-13 3.9176794e-06 9.9999619e-01], Uncertainty (std) = [1.8423995e-12 1.5824246e-05 1.5819051e-05]\n",
      "Sample 59: Predicted Label = 2, Probabilities = [9.651070e-12 5.828685e-06 9.999943e-01], Uncertainty (std) = [5.2087241e-11 2.6780188e-05 2.6779253e-05]\n",
      "Sample 60: Predicted Label = 1, Probabilities = [3.5167042e-05 9.9994475e-01 2.0150392e-05], Uncertainty (std) = [1.8464820e-04 2.0426787e-04 9.3529990e-05]\n",
      "Sample 61: Predicted Label = 0, Probabilities = [9.9999952e-01 4.7725621e-07 4.8832146e-13], Uncertainty (std) = [2.0161531e-06 2.0163502e-06 3.5902234e-12]\n",
      "Sample 62: Predicted Label = 1, Probabilities = [6.0838525e-04 9.9903381e-01 3.5796198e-04], Uncertainty (std) = [0.00261026 0.00315009 0.00181019]\n",
      "Sample 63: Predicted Label = 2, Probabilities = [2.963814e-11 8.644154e-03 9.913561e-01], Uncertainty (std) = [2.5746516e-10 6.0574871e-02 6.0574889e-02]\n",
      "Sample 64: Predicted Label = 2, Probabilities = [1.5711925e-11 4.1300050e-06 9.9999601e-01], Uncertainty (std) = [9.1089379e-11 1.8710905e-05 1.8710269e-05]\n",
      "Sample 65: Predicted Label = 2, Probabilities = [7.661081e-13 3.216911e-06 9.999969e-01], Uncertainty (std) = [4.9592587e-12 2.0836849e-05 2.0837342e-05]\n",
      "Sample 66: Predicted Label = 1, Probabilities = [4.3998237e-04 9.9922395e-01 3.3605241e-04], Uncertainty (std) = [0.001232   0.00132869 0.00051336]\n",
      "Sample 67: Predicted Label = 2, Probabilities = [1.5399430e-11 9.8786923e-06 9.9999017e-01], Uncertainty (std) = [1.5043113e-10 7.7697834e-05 7.7698198e-05]\n",
      "Sample 68: Predicted Label = 1, Probabilities = [3.852764e-04 9.993081e-01 3.065519e-04], Uncertainty (std) = [0.0007436  0.00104462 0.00071005]\n",
      "Sample 69: Predicted Label = 0, Probabilities = [9.9999887e-01 1.3007641e-06 1.2980922e-11], Uncertainty (std) = [8.7156013e-06 8.7131084e-06 9.1707315e-11]\n",
      "Sample 70: Predicted Label = 2, Probabilities = [8.106464e-13 5.966476e-06 9.999942e-01], Uncertainty (std) = [4.8872282e-12 3.1130236e-05 3.1133757e-05]\n",
      "Sample 71: Predicted Label = 2, Probabilities = [9.260904e-12 5.540345e-06 9.999946e-01], Uncertainty (std) = [5.2399501e-11 2.3416773e-05 2.3413961e-05]\n",
      "Sample 72: Predicted Label = 1, Probabilities = [2.3879469e-04 9.9928319e-01 4.7802369e-04], Uncertainty (std) = [0.00053966 0.00130669 0.00112561]\n",
      "Sample 73: Predicted Label = 2, Probabilities = [6.440655e-11 7.341188e-05 9.999266e-01], Uncertainty (std) = [6.4034783e-10 5.3885637e-04 5.3886144e-04]\n",
      "Sample 74: Predicted Label = 1, Probabilities = [3.2306020e-04 9.9914217e-01 5.3479808e-04], Uncertainty (std) = [0.00054109 0.00157418 0.00151636]\n",
      "Sample 75: Predicted Label = 0, Probabilities = [9.9999976e-01 2.5640057e-07 1.6876981e-15], Uncertainty (std) = [1.9392314e-06 1.9382082e-06 6.4036017e-15]\n",
      "Sample 76: Predicted Label = 0, Probabilities = [9.9999970e-01 3.1703993e-07 1.0082244e-12], Uncertainty (std) = [1.2680530e-06 1.2646066e-06 6.4489256e-12]\n",
      "Sample 77: Predicted Label = 0, Probabilities = [9.9999982e-01 2.5893414e-07 1.5107316e-13], Uncertainty (std) = [1.1265751e-06 1.1246797e-06 9.4539676e-13]\n",
      "Sample 78: Predicted Label = 1, Probabilities = [2.880724e-04 9.993103e-01 4.014876e-04], Uncertainty (std) = [0.00049322 0.00119255 0.00102337]\n",
      "Sample 79: Predicted Label = 1, Probabilities = [6.4831227e-04 9.9917823e-01 1.7358299e-04], Uncertainty (std) = [0.0020488  0.00205685 0.00032821]\n",
      "Sample 80: Predicted Label = 2, Probabilities = [3.7599925e-13 5.8442536e-05 9.9994171e-01], Uncertainty (std) = [3.0510809e-12 5.7083450e-04 5.7083543e-04]\n",
      "Sample 81: Predicted Label = 1, Probabilities = [9.835016e-05 9.905019e-01 9.399662e-03], Uncertainty (std) = [0.00076941 0.0582096  0.05822032]\n",
      "Sample 82: Predicted Label = 2, Probabilities = [2.2883427e-12 3.0186941e-06 9.9999708e-01], Uncertainty (std) = [1.0505972e-11 1.0304989e-05 1.0307220e-05]\n",
      "Sample 83: Predicted Label = 1, Probabilities = [4.0148559e-04 9.9918532e-01 4.1330184e-04], Uncertainty (std) = [0.0010675  0.00156405 0.00121044]\n",
      "Sample 84: Predicted Label = 1, Probabilities = [3.8314174e-04 9.9881601e-01 8.0071524e-04], Uncertainty (std) = [0.00088261 0.00428433 0.00422814]\n",
      "Sample 85: Predicted Label = 0, Probabilities = [9.9999970e-01 4.4432829e-07 1.3056548e-14], Uncertainty (std) = [2.6384062e-06 2.6377104e-06 7.1463811e-14]\n",
      "Sample 86: Predicted Label = 0, Probabilities = [9.9999881e-01 1.3024812e-06 1.7423434e-13], Uncertainty (std) = [6.9218627e-06 6.9170296e-06 9.2044222e-13]\n",
      "Sample 87: Predicted Label = 0, Probabilities = [9.9999845e-01 1.5893370e-06 5.7479776e-12], Uncertainty (std) = [6.8181357e-06 6.8209092e-06 4.0416351e-11]\n",
      "Sample 88: Predicted Label = 1, Probabilities = [7.9338672e-04 9.9850184e-01 7.0450740e-04], Uncertainty (std) = [0.00358014 0.00397527 0.00188209]\n",
      "Sample 89: Predicted Label = 0, Probabilities = [9.9999923e-01 9.1482025e-07 1.6935952e-12], Uncertainty (std) = [3.160710e-06 3.154242e-06 9.928405e-12]\n",
      "Sample 90: Predicted Label = 0, Probabilities = [9.9999946e-01 6.5569577e-07 4.4821663e-13], Uncertainty (std) = [1.8559550e-06 1.8482497e-06 2.5170161e-12]\n",
      "Sample 91: Predicted Label = 2, Probabilities = [5.7238991e-11 1.0954978e-04 9.9989057e-01], Uncertainty (std) = [4.3368931e-10 6.4128399e-04 6.4128253e-04]\n",
      "Sample 92: Predicted Label = 1, Probabilities = [3.7295837e-04 9.9945223e-01 1.7457112e-04], Uncertainty (std) = [0.00084166 0.00087342 0.00030583]\n",
      "Sample 93: Predicted Label = 2, Probabilities = [1.1424733e-12 1.2563416e-06 9.9999893e-01], Uncertainty (std) = [6.5072388e-12 4.0682894e-06 4.0751970e-06]\n",
      "Sample 94: Predicted Label = 1, Probabilities = [3.9815641e-04 9.9956125e-01 4.0611580e-05], Uncertainty (std) = [0.00246039 0.00246143 0.0001916 ]\n",
      "Sample 95: Predicted Label = 2, Probabilities = [1.3772792e-12 2.3487123e-06 9.9999779e-01], Uncertainty (std) = [1.1582551e-11 5.4312864e-06 5.4376555e-06]\n",
      "Sample 96: Predicted Label = 0, Probabilities = [9.9999982e-01 2.2767489e-07 4.5313620e-13], Uncertainty (std) = [7.4950322e-07 7.4367148e-07 2.2586653e-12]\n",
      "Sample 97: Predicted Label = 0, Probabilities = [9.9999940e-01 7.1440206e-07 2.8057998e-13], Uncertainty (std) = [3.6218246e-06 3.6180008e-06 1.5055957e-12]\n",
      "Sample 98: Predicted Label = 0, Probabilities = [9.999997e-01 3.855301e-07 5.671067e-13], Uncertainty (std) = [1.1442229e-06 1.1378764e-06 2.1716166e-12]\n",
      "Sample 99: Predicted Label = 0, Probabilities = [9.9998987e-01 1.0252755e-05 1.6497429e-14], Uncertainty (std) = [9.8275057e-05 9.8271586e-05 1.4329742e-13]\n"
     ]
    }
   ],
   "source": [
    "# Example output with probabilities and uncertainty\n",
    "for i, (mean, std) in enumerate(zip(mean_predictions, std_predictions)):\n",
    "    # softmax_probs = np.exp(mean) / np.sum(np.exp(mean)) # Softmax to get probabilities\n",
    "    print(f'Sample {i}: Predicted Label = {predicted_labels[i]}, Probabilities = {mean}, Uncertainty (std) = {std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Export the reults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Save test predictions to a CSV\n",
    "test_df = pd.DataFrame({\n",
    "    'Prediction': predicted_labels,\n",
    "    'Probability_0': [p[0] for p in mean_predictions],\n",
    "    'Probability_1': [p[1] for p in mean_predictions],\n",
    "    'Probability_2': [p[2] for p in mean_predictions],  # Adjust based on num_classes\n",
    "    'Uncertainty_0': [u[0] for u in std_predictions],\n",
    "    'Uncertainty_1': [u[1] for u in std_predictions],\n",
    "    'Uncertainty_2': [u[2] for u in std_predictions] \n",
    "})\n",
    "\n",
    "test_df.to_csv('test_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "save(ae_attention_model.state_dict(), 'autoencoder_attention_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('ae_data.h5', 'w') as f:\n",
    "    f.create_dataset('X', data=data_module.val_features)\n",
    "    f.create_dataset('y', data=data_module.val_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainable AI Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('ae_data.h5', 'r') as f:\n",
    "    X = f['X'][:]\n",
    "    y = f['y'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_autoencoder_classifiers import AutoencoderAttentionClassifier\n",
    "from pl_model_utils import TimeSeriesDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TimeSeriesDataset(\n",
    "            torch.tensor(y, dtype=torch.float32), \n",
    "            torch.tensor(X, dtype=torch.float32)\n",
    "            )\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_loader))\n",
    "targets, features = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = AutoencoderAttentionClassifier(\n",
    "    context_length=1, \n",
    "    num_classes=3, \n",
    "    num_features=2,\n",
    "    lr=0.001,\n",
    "    num_heads=1,\n",
    "    dropout_prob=0.1,\n",
    "    hidden_units=256,\n",
    "    embed_dim=128,\n",
    "    classifier_units=64)\n",
    "ae_model.load_state_dict(torch.load('autoencoder_attention_classifier.pth'))\n",
    "ae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "e = shap.DeepExplainer(ae_model, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import modeling_tf_utils"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GluonTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
